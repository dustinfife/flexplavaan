---
title: "flexplavaan_draft"
output:
  bookdown::pdf_document2:
      css: style.css
      number_sections: false
bibliography: flexplavaan_citations.bib
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE, note=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, note=FALSE)
require(tidyverse)
```

# Introduction
It is currently an unprecedented time in the social sciences; multiple scientific disciplines are reeling from a "replication crisis" [@Camerer2018; @Ioannidis2005; @Pashler2012a], new norms for credibility are becoming more prevalent (Nelson, Simmons, & Simonsohn, 2018; Nosek, Ebersole, DeHaven, & Mellor, 2018), and the push for open science is accelerating at a rapid pace [@Nosek2018]. Amidst this push for open science practices, some have called for greater use of visualization techniques [@Fife2019a; @Fife2019e; @Tay2016a]. As noted by Tay, et al. (2016), "[visualizations]...can strengthen the quality of research by further increasing the transparency of data..." (p. 694). In other words, one of the best, and most efficient ways of making data analysis open and transparent is to display each and every data point through visualization techniques. This is particularly important in research applications where participant-level data cannot be shared.

Not only do visualizations adhere to the principles of openness and transparency, but they offer several additional advantages; they vastly improve encoding of information [@Correll2015], they highlight model misfit [@Healy2014a], and they are an essential component in evaluating model assumptions [@Levine2018; @Tay2016a]. As such, we [as well as others, e.g., @Fife2019b; @Fife2019e; @Wilkinson1999a] insist that every statistical model ought to be accompanied by a graphic. 

Unfortunately, this visualization requirement is easier said than done. While visualizing some statistical models is trivial (e.g., regressions, t-tests, ANOVAs, multiple regression), visualizing others is not. One particularly troublesome class of models to visualize is latent variable models (LVMs). While researcher routinely visualize conceptual models (e.g., via path diagrams), visualizing the statistical models is not so easy. The former visualizations are common, while the latter are not [@Hallgren2019a]. The reason statistical visualizations of LVM are not intuitive is because they rely on unobserved variables [@bollen_structural_1989]. If the variables of interest are unobserved, how can we possibly visualize them? 

Though it is not, at first glance, easy to visualize unobserved variables, that does not mean visualizing them is any less important. On the contrary, visualizing latent variables is, perhaps more important because their presence is unobserved. In the following section, we elaborate on why visualizations are particularly crucial for LVMs. We then review previous approaches others have used for visualizing LVMs, and note their strengths and weaknesses. We then introduce our approach and the corresponding R package flexplavaan, which allows users to visualize both lavaan and blavaan objects in R. We then conclude with several examples that highlight how visualizations assisted in identifying appropriate statistical models. 

# The Role of Visualizations in Structural Equation Models 
The primary argument we proffer in this paper is that post-estimation visualization strategies should be a standard part of SEM evaluation. Visualization strategies can:

* highlight model misspecifications that might otherwise go undiscovered
* aid in identification of remedial strategies
* assess modeling assumptions
* improve model interpretation

We provide an R package, *flexplavaan*, which can be readily used in conjuction with the *lavaan* SEM package. 

The validity of SEM-based inferences is predicated on the specification of structural models that closely approximate real-world causal processes [@bollen2019model; @Hayduk2014]. Misspecifications in any one part of the model can lead to biases that spread throughout the full systems of equations [@bollen2019model]. When evaluating model adequacy, SEM practitioners generally rely on global fit tests and approximate fit indices to evaluate the degree to which model-implied variances, covariances, and means deviate from the observed data [@jackson2009reporting]. An important limitation of global fit indices, however, is that models can yield desirable values (e.g., a non-significant $\chi^2$ test), indicating a strong *overall* model-data correspondence, even when specific aspects of the model are misspecified [@Goodboy2017; @Hayduk2014; @tomarken2003potential]. Moreover, in our experience, applied users often lack an intuitive understanding of what global fit statistics tell them about their models. Rather than using these indices to identify  problematic model features, users often rely on conventional cutoffs [e.g, @hu1998fit] to determine whether a model is "adequate", a practice that has received resounding criticism [@barrett2007structural; @Chen2008; @hayduk2014shame; @McIntosh2007]. This lack of understanding is evident when applied users express confusion about why global fit indices that approach model evaluation in different ways (e.g., the root mean square error of approximation and the comparative fit index) provide conflicting assessments of model fit [@lai2016problem]. 

A number of scholars have emphasized the importance of supplementing SEM global fit indices with local fit assessment, investigating the tenability of all specific model implications individually [@bollen2019model; @Goodboy2017; @hayduk2014seeing; @Thoemmes2018; @tomarken2003potential; @Tomarken2005]. This is consistent with the causal graph approach which emphasizes the nonparametric identification of all causal implications (e.g., conditional independencies among pairs of variables) prior to model estimation [@pearl2012causal]. Local fit evaluation procedures (e.g., inspection of residual correlation matrices [@bollen_structural_1989], confirmatory tetrad analysis [@hipp2003model], and equation-based overidentification tests [@bollen2019model]) can help identify individual model specifications that are inconsistent with the data and may give clues about effective remedial strategies [@bollen_structural_1989; @Goodboy2017; @tomarken2003potential; @Tomarken2005]. For example, in a congeneric confirmatory factor analysis (CFA) model, inspection of the residual covariance or correlation matrices could uncover evidence of correlations among indicator disturbances that, according to the model, should be independent once conditioning on the latent factor. However, reliance on numeric indicators of local misfit could be problematic. When evaluating the residual correlation matrix in a congeneric factor model, for instance, a nonlinear (e.g., quadratic) relationship among indicator disturbances may yield a small (linear) residual correlation, masking a meaningful model misspecification that could be clearly identified using visual strategies. Furthermore, visual inspection of model-data correspondence may help identify patterns of misspecifications (e.g., systematic overestimation of the effects of a latent factor and its indicators) more readily than inspection of a residual correlation matrix.  
 
Perhaps most importantly, we find that applied SEM users are often unaware of the assumptions underlying their SEMs and the necessary conditions implied by their model structures. We regularly encounter applied SEM users who are, for example, unaware of the -- rather stringent -- requirement of the classic CFA model that all latent error variables are independent conditional on the latent factor (i.e., the local independence) [@reise1993confirmatory]. The availability of non-intuitive numeric fit indices with arbitrary thresholds for "acceptable" models may enable applied users to implement SEMs without a satisfactory  understanding of core principles, relying on fit indices to identify model inadequacies. If applied users were expected to provide graphical displays showing the degree to which their observed data are commensurate with their model's implications, we expect that this would force them to attend to their model's implications and (hopefully) acquire a better understanding of the structural implications of their models.

# Previous Approaches to Visualizing LVMs
There is sparse literature describing visual strategies for evaluating the adequacy of SEMs relative to the extensive literature discussing the merits of global fit tests and indices [e.g., @barrett2007structural; @Chen2008; @hayduk2014shame; @hu1998fit; @McIntosh2007; @shi2020effect; @smith2001primer; @Steiger2007]. While reporting of numeric fit indices is ubiquitous in psychological applications SEM applications [@jackson2009reporting], we rarely see plots investigating model adequacy. There is, however, some scholarship demonstrating the utility of visual procedures for evaluating the tenability of model assumptions, diagnosing causal misspecifications, and selecting the best model from a group of competitors.  

A common approach for identifying model-data discrepancies is to plot the distribution of the residual covariances/correlations (e.g., using stem-and-leaf plots or histograms) [@bollen_structural_1989]. This can aid in identifying the specific model implications that are most inconsistent with the observed data [@bollen_structural_1989; @bollen1991observational]. Residual covariances may alert modelers to problems that would otherwise otherwise go undetected when models achieve desirable values on global fit indices [@Goodboy2017; @tomarken2003potential; @Tomarken2005]. Distributional plots of residual covariances may be particularly helpful for identifying aberrant residual covariances and systematic patterns of misspecification (e.g., consistent under- or over-estimation of correlations) in complex models where the residual covariance matrix has many elements. 

Global fit indices and residual covariances, on account of their focus on aggregate statistics, do not identify aberrant cases that may influence model parameter estimates and fit [@bollen1991observational]. @bollen1991observational demonstrated methods of calculating raw and standardized individual case residuals (ICRs) representing differences between observed and model-estimated case values for outcome variables [@bollen1991observational]. They used stem-and-leaf, index, and histogram plots to help locate outlying and influential observations. @pek2011sensitivity demonstrated how case diagnostic procedures commonly used in generalized linear models (e.g., Mahalanobis distance, generalized Cook’s D, and DFBETAs) can be applied to SEMs to detect influential cases with index plots clearly identifying the most aberrant cases. @flora2012old applied these diagnostic procedures and others specifically to factor analysis models, and @yuan2010fitting used visualizations of Mahalanobis distance metrics to identify high-leverage cases and outliers. Visualization procedures showing case influence on model fit (e.g., likelihood differences) and parameter estimations (e.g., generalized Cook’s D) have been implemented in open-source R packages, including faoutlier [@chalmers2017package] and influence.SEM [@pastore2018package]. 

@Asparouhov2017 used estimated factor scores and ICRs to detect specific structural misspecifications. First, they showed that plots of estimated factor scores for a latent outcome variable against observed predictor variables can be used to detect unspecified nonlinear effects of the predictor on the latent outcome. Second, they used ICR scatterplots to detect violations of local independence in a congeneric latent factor model. Finally, they demonstrated in a latent factor model how plotting predicted values for a reflective indicator against the observed indicator values could aid in uncovering unmodeled heterogeneity that could be better captured using a mixture model. 

@raykov2014latent used visualizations of ICRs to aid in model selection in the context of latent growth curve modeling. When comparing linear and quadratic growth curve models for the same data, for example, they showed that a scatterplot of the ICRs for the quadratic model vs. ICRs for a linear model can help identify which model best minimizes model-data discrepancies. In the context of growth mixture modeling, @wang2005residual showed how visualization of empirical Bayes residuals (e.g., Q-Q and trajectory plots) can aid in determining the appropriate number of classes, an adequate shape of within-class growth trajectories, and missing confounders.  

A potential limitation of visual model-evaluation procedures based on ICRs is that they rely on factor score estimates. Individual latent factor scores cannot be uniquely determined [@grice2001computing; @rigdon2019factor; @steiger1996dispelling]. In cases where factors are highly indeterminate -- e.g., factors with few indicators only weakly predicted by the latent factor -- different factor score estimation methods can yield highly discrepant values, potentially even estimates that are negatively correlated [@grice2001computing]. Therefore, in the presence of high levels of factor indeterminacy, the conclusions drawn from visual diagnostic strategies using individual scores could vary depending on which factor score estimation strategy is employed.

# Our Approach (Linear LVMs)

## Diagnostic Plots: Trail Plots

To begin how to conceptualize LVMs, let us first consider how typical linear models are visualized. In a standard regression, each dot in a scatterplot represents scores on the observed variables. Often, analysts overlay additional symbols to represent the fit of the model (e.g., a line to represented the fitted regression model, or large dots to represent the mean). Sometimes additional symbols are overlaid to represent uncertainty (e.g., confidence bands for a regression line or standard error bars). See Figure \@ref(fig:scatter) as an example. In either case, the dots represent observed information, while the fitted information is conveyed using other symbols. 

```{r scatter, fig.cap="Example figure that shows how standard statistical models are visualized. Dots represent scores on observed variables, while other symbols (e.g., regression line, large dots) represent the fit of the model."}
require(flexplot)
a = flexplot(weight.loss~therapy.type, data=exercise_data) + labs(x="Therapy", y="Weight Loss")
b = flexplot(weight.loss~motivation, data=exercise_data) + labs(x="Motivation", y="Weight Loss")
cowplot::plot_grid(a,b)
```

Likewise, visualizing LVMs ought to follow similar conventions; the dots should represent the observed information, as in @Bauer2005. In his visuals, pairwise relationships between observed variables are represented in a scatterplot. However, Bauer’s approach did not overlay a model-implied fit, as we seek to do. When the line represents the model-implied fit, it denotes the trail left behind by the unobserved latent variable. As such, we call these plots "trail plots."
How then does one identify the slope/intercept of the LVM’s model-implied fit? It is quite easy to do so when standard linear LVMs are used. Suppose we have a factor ($T$) with three indicators (e.g., $X_1, X_2$ and $X_3$), and we wish to visualize the pairwise trace plot between $X_1$ and $X_2$. To do so, we can simply utilize the model-implied correlation matrix:

$$\beta_{x_1|x_2} = \hat{r}_{x_1,x_2}\frac{s_{x_1}}{s_{x_2}}$$   

where $\hat{r}_(x_1,x_2)$ is the model-implied correlation between $X_1$ and $X_2$, $s_{x_1}$ and $s_{x_2}$ are the standard deviations of the two variables. One can then estimate the intercept using basic algebra:

$$b_0=\bar{X}_1-\beta_(x_1|x_2)\bar{X}_2$$

```{r}
require(lavaan)
require(tidyverse)
set.seed(1212)
n = 500
slopes = c(.75, .75, .75)
latent = rnorm(n)

# create three indicators
x1 = slopes[1]*latent + rnorm(n,0, sqrt(1-slopes[1]^2))
x2 = slopes[2]*latent + rnorm(n,0, sqrt(1-slopes[2]^2))
x3 = slopes[3]*latent + rnorm(n,0, sqrt(1-slopes[3]^2))
d = data.frame(x1=x1, x2=x2, x3=x3)

# model in lavaan
model.linear = '
  A =~ x1 + x2 + x3
  A ~~ A
'
fitted = cfa(model.linear, data=d)
```


Figure \@ref(fig:trace1) shows the LVM model-implied fit in red with a regression line in blue for simulated data. Because the regression line minimizes the sum of squared errors, we would hope that the LVM fitted line (red) closely approximate the regression line. In this case, the two overlap quite extensively. On the other hand, if the two lines differ, we can be certain the LVM fails to capture the entire relationship between the two observed variables. 

```{r trace1, fig.cap = "The LVM-implied fit between X1 and X2, shown in red. The blue line represents the regression line between the two variables. The more closely the model-implied fit line resembles the regresson line, the better the fit of the LVM."}
  # compute the model-implied slope
implied.cor = lavInspect(fitted, what="cor.ov")
implied.cov = lavInspect(fitted, what="cov.ov")
stdev_ov = sqrt(diag(implied.cov))
estimated.slope = implied.cor["x1","x2"]*(stdev_ov["x2"]/stdev_ov["x1"])

ggplot(data=d, aes(x=x1,y=x2)) +
  geom_point() +
  geom_abline(slope = estimated.slope, intercept=0, col="red") +
  geom_smooth(method="lm") +
  labs(x="Exam One", y="Exam Two") +
  theme_bw()
```

 

Of course, Figure \@ref(fig:trace1) only shows one pairwise relationship between variables. If we wished to visualize all the variables in our model, we would have to utilize a scatterplot matrix, as in Figure \@ref(fig:traceMatrix). Naturally, this becomes quite cumbersome when users have more than seven or eight variables. In this case, it is best to visualize only a subset of variables. We will later discuss strategies for how best to select appropriate subsets. 

```{r traceMatrix, fig.cap="Scatterplot matrix showing the model-implied fit (red) and regression-implied fit (blue) between three simulated indicator variables. The diagonals show the histograms."}
require(flexplavaan)
visualize(fitted, plot = "model", sample=300)
```


The primary advantage of trail plots is that they easily show misfit in LVMs. They do so by showing differences in fit between indicators of different variables. For example, \@ref(fig:crossloading) shows a model where $x1-x2$ load onto one latent variable, x4-x5 load onto another, and x3 loads onto both; however, the specified model assumes x3 only loads onto the first model. 

```{r crossloading, fig.cap="This plot shows structural misfit; x3 loads onto two factors, but only one is modeled."}
knitr::include_graphics("../plots/crossloading.jpg")
```

Another advantage of trail plots is they visually (and often times strikingly) show how little information a model might capture. For example, Figure \@ref(fig:poorfit) shows a model that, by the fit indices, has remarkably good fit: RMSEA is <0.001, TLI/CFI/NNFI are all approximately 1, and RMR is approximately zero. However, the visual shows the model actually captures very little information; while the two lines are quite similar, the majority of slopes are very near flat. 

```{r poorfit, fig.cap="This plot shows a model that, by standard fit indices, fits quite well. The visuals, on the other hand, illustrate that the model isn't capturing"}
knitr::include_graphics("../plots/poorfit.jpg")
```

# Disturbance-Dependence Plots
	
One common technique for visualizing the adequacy of statistical models in classic regression is residual-dependence plots. With these graphics, one simply plots the residuals of the model ($Y$ axis) against the predicted values ($X$ axis). The rationale behind this is simple: the model should have extracted any association between the prediction and the outcome. The residuals represent the remaining information after extracting the signal from the model. If there is a clear trend remaining in the data (e.g., a nonlinear pattern or a "megaphone" shape in the residuals), this indicates the model failed to capture important information.
	
Likewise, in LVMs, we can apply this same idea to determine whether the fit implied by the LVM has successfully extracted any association between any pair of predictors. However, in LVMs, residuals refer to the discrepancy between the model-implied and the actual variance/covariance matrix (or correlation matrix). As such, naming these plots "residual-dependence plots" would be a misnomer. Rather, misfit at the raw data level is typically called a disturbance. As such, we call these plots disturbance-dependence plots.

Like trace plots, we visualize disturbance-dependence plots for each pair of observed variables. To do so, `flexplavaan` subtracts the fit implied by the trace plots from the observed scores. For example, a disturbance dependence plot for an $X_1/X_2$ relationship would subtract the "fit" of $X_2$ implied by the trace plot from the actual $X_2$ scores (and vice versa for the $X_2/X_1$ relationship). If the trace-plot fit actually extracts all association between the pair of observed variables, we would expect to see a scatterplot that shows no remaining association between the two. If there is a pattern in the scatterplot remaining, we know the fit of the model misses important information about that specific relationship. To aid in interpreting these plots, we can overlay the plot with a flat line (with a slope of zero), as well as a regression (or loess) line. The first line indicates what signal should remain after fitting the model, while the second line shows what actually remains. 
Figure \@ref(fig:ddp) shows an example of trace plots in the upper triangle and disturbance-dependence plots in the lower triangle of a scatterplot matrix. These plots are for the same data shown in the right image of Figure \@ref(fig:crossloading). Notice how the plots associated with $X_3$ all have positive slopes, indicating the model failed to capture important signal remaining in the model. 

(ref:crossloadcitation) This plot shows a disturbance-dependence plot for the same data visualized in Figure \@ref(fig:crossloading) in the lower triangle.

```{r ddp, fig.cap='(ref:crossloadcitation)'}
knitr::include_graphics("../plots/crossloading_ddp.jpg")
```

Together, both of these plots (trace plots and diagnostic-dependence plots) serve as a critical diagnostic check. Both these plots will signal misfit both in the measurement and structural components of the model. Conversely, these models will also help users determine whether the model is to be believed. If they show the model adequately fits the data, the user can then proceed to plot two different types of plots: measurement plots and structural plots. 

# Measurement Plots

One of the primary purposes of the diagnostics is to determine whether one's conceptualization of the latent variables is to be believed. If the trace plots and disturbance dependence plots indicate the LVM is a good representation of the data, one can be more confident the latent variables are properly estimated. If this is the case, we can now make a step toward visualizing the latent variables themselves. 

The approach we suggest is to plot the factor scores on the $Y$ axis and the observed scores on the $X$ axis. Naturally, this means one could really only visualize one indicator at a time, which is a serious limitation for most (if not all) latent variable models. To overcome this problem, we recommend paneling each indicator, as in Figure \@ref(fig:measurementplot). To do so, this requires converting our data from "wide" to "long" format (see Table \@ref(tab:long)), meaning that variables that once occupied separate columns (e.g., $x1, x2, \text{ and } x3$; see Table \@ref(tab:wide)) are now collapsed into two columns, one of which contains the values of $x1-x3$, the other of which indicates which variable the measurement belongs to (see Table \@ref(tab:long)). We should also convert each variable to $z$-scores to make it easier to compare the observed/latent relationship across indicators, as in Table \@ref(tab:long). 

```{r wide, results="asis"}
x1 = round(rnorm(3, 10, 2))
x2 = round(rnorm(3, 10, 2))
x3 = round(rnorm(3, 10, 2))
k = data.frame(x1=x1, x2=x2, x3=x3)
knitr::kable(k, caption = "A Table of Simulated Data Showing the Data in Wide Format")
```

(ref:tab1ref) This Table Contains the Same Data in Table \@ref(tab:wide), but standardized and in "Long" Rather Than "Wide" Format.
```{r long}
require(tidyverse)
k2 = k %>% mutate(x1 = scale(x1), x2=scale(x2), x3=scale(x3)) %>% gather(key="Measure", value="Observed", x1:x3) 
knitr::kable(k2, caption='(ref:tab1ref)')
```

Another alteration from a standard scatterplot is the use of vertical bars to indicate uncertainty in estimating the latent variables. In Figure \@ref(measurementplot), the dots represent the estimated factor scores and the lines indicate $\pm 1 SD$. The red line is a "ghost line" [@Fife2019c], which simply repeats the line from the second panel ($x2$) to the other panels. This makes it easier to identify which indicators best load onto the latent variable (i.e., which indicator is most reliable). In Figure \@ref(fig:measurementplot), $x2$ has the largest factor score. 

(ref:meas) This image, called a measurement plot, shows the relationship between the latent variable ($Y$ axis) and each of the standarized indicators ($x1, x2$, and $x3$).

```{r measurementplot, fig.cap='(ref:meas)'}
# extract factor score estimates
d[,c("latent")] = lavPredict(fitted)
# convert to long format
measurement_data = d %>% 
  mutate_at(vars(x1:x3), scale) %>% # convert to standardized variables
  gather(key="Measure", value="Observed", x1:x3) # convert to long format
# plot it

require(semTools)
se_posterior = plausibleValues(fitted)
se_posterior = se_posterior %>% tibble() %>% unnest() %>% 
  group_by(case.idx) %>% 
    summarize(sd_imp = sd(A))
measurement_data$se = se_posterior$sd_imp

flexplot(latent~Observed | Measure, 
         data=measurement_data, 
         method="lm", 
         ghost.line = "red", alpha=.4) + 
        geom_errorbar(aes(ymin=latent-se, ymax=latent+se), alpha = .2)

```

Naturally, there is a great deal of overlapping points/lines in Figure \@ref(fig:measurementplot). To minimize overlap, we can instead sample datapoints, as in Figure \@ref(fig:sampledMeasurement). 

```{r sampledMeasurement}
m2 = sample(1:nrow(measurement_data), size=100)
m2 = measurement_data[m2,]
flexplot(latent~Observed | Measure, 
         data=m2, 
         method="lm", 
         ghost.line = "red", alpha=.4) + 
        geom_errorbar(aes(ymin=latent-se, ymax=latent+se), alpha = .2)
```

# Structural (Beech Ball) Plots

When modeling latent variables, often the visuals of interest are not the observed variables, but the latent variables. In other words, the measurement model is ancillary to the substantive model. Naturally, we might wish to visualize the relationship between the latent variables.

However, as before, we need to have visuals that reflect uncertainty in our estimates of the latent scores. With measurement plots, we needed only to reflect that uncertainty on the $Y$ axis (because the $Y$ axis displayed the latent variable while the $X$ axes dispalyed the observed variables). When visualizing relationships between latent variables, on the other hand, both axes should reflect uncertainty. In `flexplavaan`, this uncertainty is represented as ellipses. The diameter of the ellipses (for both the $X$ axis and the $Y$ axis) are obtained from prediction intervals for `lavaan` objects or from posterior distributions for `blavaan` objects. Figure \@ref(fig:beech) shows these plots, which we call "Structural Plots," or "Beech Ball Plots" (because the ellipses look like beech balls in various stages of compression). 

```{r beech, fig.cap='Structural or "Beech Ball" plot of the relationship between the latent variables Force and Jedi. The ellipses represent the prediction intervals for the factor scores of the latent variables.', echo=FALSE}
require(lavaan)
require(flexplot)
require(tidyverse)
data(jedi_jedi)

## specify model
model_cross = "
force_score =~ fitness + saber + midichlorian + force_history
jedi_score =~ exam_one + exam_two + exam_three + force_history
jedi_score ~ force_score
"
# Fit the models ----------------------------------------------------------
force_cross = cfa(model_cross, jedi_jedi)



# extract factor score estimates
jedi_jedi[,c("force_score", "jedi_score")] = lavPredict(force_cross)


require(semTools)
se_posterior = plausibleValues(force_cross)
se_posterior = se_posterior %>% tibble() %>% unnest() %>% 
  group_by(case.idx) %>% 
    summarize(sd_force = sd(force_score),
              sd_jedi = sd(jedi_score))
jedi_jedi$sd_force = se_posterior$sd_force
jedi_jedi$sd_jedi = se_posterior$sd_jedi

m2 = sample(1:nrow(jedi_jedi), size=500)
m2 = jedi_jedi[m2,]

require(ggforce)
flexplot(force_score~jedi_score, 
         data=m2, 
         method="loess", 
         se=F,
         ghost.line = "red", alpha=0) + 
        ggforce::geom_ellipse(aes(x0 = jedi_score, y0 = force_score, a = sd_jedi, b = sd_force, angle=0), color=rgb(0,0,0,.1))

```

Aside from the beech balls, there is a great deal of flexibility in how one visualizes the structural model. In our example, we only had two variables to visualize, so a simple bivariate plot was most natural. When more variables are included, we might utilize paneling, added variable plots, beeswarm plots, etc. For a review of the types of plots possible, see @Fife2019c. 


# References