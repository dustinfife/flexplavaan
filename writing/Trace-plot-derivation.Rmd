---
title: "Seeing the Impossible: Visualizing Latent Variable Models"
header-includes:
   - \usepackage{tikz}
   - \usetikzlibrary{shapes.geometric,arrows}
   - \usepackage{amsmath}
output:
  pdf_document
bibliography: ../references.bib
---


```{r, echo=FALSE, message=FALSE, warning=FALSE}
require(knitr)
require(GGally)
require(flexplot)
opts_chunk$set(message=FALSE, echo = TRUE, warning = FALSE)
source('../R/scratch/archive/ggally_pairs_function.R')
```
# Brief Introduction

SEM is a powerful tool that is infinitely flexible and able to handle diverse sorts of modeling procedures. However, it has two fatal flaws. First, latent variables are, by definition, unobserved. As such, data visualization, a critical component of model building is not an easy task. Although some have advocated for the use of scatterplots of latent variables [@Hallgren2019], these plots assume the model has accurately estimated the parameters, which may or may not be the case. Second, standard latent variable models are estimated from summary statistics, such as means and covariances. These summary statistics may be misleading if model assumptions have been violated (e.g., if the data are not normally distributed or if relationships are nonlinear). Although there are tools that allow users to *model* nonnormal data and nonlinear patterns, few *diagnostic* tools exist that allow users to assess the degree to which assumptions have been met (and/or whether violations of these assumptions are problematic). 

To overcome both of these limitation, we develop visualization tools designed for latent variable models. These tools will not only provide diagnostic checks, but they will allow both researchers and lay audiences to intuitively understand the fit of latent variable models. 

# Diagnostic Plots

## Trace Plots: Estimating the Model-Implied Slope

Suppose we have the factor analysis model shown below. 

\begin{center}
\begin{tikzpicture}[scale=1] 
\def\x{latex}
\draw (0,3) node[ellipse, minimum height=1cm,minimum width=1.5cm,draw] {$F$};
\draw (-2.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {$x1$};
\draw (0,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {$x2$};
\draw (2.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {$x1$};
\draw[arrows={-\x}] (0,2.5) --(-2.5,.5) node[midway, above] {$a$};
\draw[arrows={-\x}] (0,2.5) --(0,.5) node[midway, left] {$b$};
\draw[arrows={-\x}] (0,2.5) --(2.5,.5) node[midway, above] {$c$};
\end{tikzpicture}
\end{center}

Assume both indicators and the latent variable are standard normal variables. 

To make the exercise more illustrative, I have simulated these data:

```{r, message=FALSE, warning=FALSE, fig.cap="Scatterplot of the x1/x2 relationship. The red line shows the implied fit between the two variables under the assumption of perfect reliability. The blue line shows the actual fitted relationship. \\label{fig:first}"}
require(lavaan)
require(tidyverse)
set.seed(1212)
n = 500
slopes = c(.75, .75, .75)
latent = rnorm(n)

# create three indicators
x1 = slopes[1]*latent + rnorm(n,0, sqrt(1-slopes[1]^2))
x2 = slopes[2]*latent + rnorm(n,0, sqrt(1-slopes[2]^2))
x3 = slopes[3]*latent + rnorm(n,0, sqrt(1-slopes[3]^2))
d = data.frame(x1=x1, x2=x2, x3=x3)

# model in lavaan
model.linear = '
  A =~ x1 + x2 + x3
  A ~~ A
'
fitted = cfa(model.linear, data=d)

```

Using Wright's tracing rules, we can reproduce the model-implied correlation matrix:

$$
\begin{pmatrix} \begin{matrix}
1 & ab & ac \\
ab & 1 & bc \\
ac & bc & 1
\end{matrix} \end{pmatrix}
$$
Such matrices are standard output in any SEM software, including lavaan and blavaan. Recall the following relationship between the correlation and the slope,

\begin{equation}
\beta = r \frac{\sigma_Y}{\sigma_X}
\nonumber
\end{equation}

This allows us to calculate the model-implied slope for each bivariate relationship:

\begin{align}
E[Y|X] &= r_{ab}\frac{\sigma_Y}{\sigma_X}X  
\label{eq:slopemodimp}
\end{align}

Overlaying Equation \ref{eq:slopemodimp} on the raw data, we get a line that closely approximates the actual relationship between the two variables (Figure \ref{fig:second}), indicating the model fits well. 

```{r, fig.cap="Scatterplot of the x1/x2 relationship, where the red line shows the implied fit between the two variables. The blue line shows the actual fitted relationship, while the red line shows the model-implied fit. \\label{fig:second}"}

  # compute the model-implied slope
implied.cor = lavInspect(fitted, what="cor.ov")
implied.cov = lavInspect(fitted, what="cov.ov")
stdev_ov = sqrt(diag(implied.cov))
estimated.slope = implied.cor["x1","x2"]*(stdev_ov["x2"]/stdev_ov["x1"])

ggplot(data=d, aes(x=x1,y=x2)) +
  geom_point() +
  geom_abline(slope = estimated.slope, intercept=0, col="red") +
  geom_smooth(method="lm") +
  labs(x="Exam One", y="Exam Two") +
  theme_bw()
```

We could also apply this command continuously to all observed variables in a scatterplot matrix (using, say, the upper diagonal), as in Figure \ref{fig:third}. Here, I am utilizing the `visualize` function in the `flexplavaan` package.  As before, the red line indicates the model-implied fit. This time, however, the blue line is the fit of a loess line (which will allow for the detection of nonlinear patterns). Notice the two lines are quite similar (as they should be because these are simulated data).  

```{r, echo=FALSE, fig.cap="Scatterplot matrix of the indicator variables. As before, the red line shows the implied fit between the two variables and the blue line shows the actual fitted relationship (measured via loess lines). \\label{fig:third}", fig.width=4, fig.height=4}
require(flexplavaan)
visualize(fitted, plot = "model")
```

## Disturbance Dependence Plots: Removing the Effect of the Latent Variable

It may also be interesting to utilize "disturbance dependence plots," or to plot the relationship between the observed variables on interest, once the effect of the latent variable(s) have been removed. This is similar to a residual dependence plot in regression models, but we use different terminology to avoid confusion (since residuals in SEM often refer to the residual correlation matrix). 

To generate these disturbance terms, we need only to generate predicted $Y$ scores from Equation \ref{eq:4} for each individual, then subtract those from the observed $Y$ scores. We can then plot the relationship between $Y$ and $X$ after removing the effect of the latent variable. If the data are locally independent, we should observe no relationship. The expected relationship (i.e., a line centered on zero with no slope) is colored as red. We could then add this plot to the scatterplot matrix as in Figure \ref{fig:five}. The default for the visualize function displays the model-implied plots in the upper triangle and the disturbance dependence plots in the lower triangle. As expected (because the data were simulated), the loess lines in the lower triangle are very similar to the model-implied line (i.e., the line where $y$=0). 

```{r, echo=FALSE, fig.cap="Scatterplot matrix of the indicator variables. The upper triangle show the model-implied fit between the indicators, while the lower triangle shows the disturbance dependence plots. As before, the red line shows the implied fit between the two variables and the blue line shows the actual fitted relationship (measured via loess lines). \\label{fig:five}", fig.width=4, fig.height=3}
visualize(fitted)
```


## Detecting Cross-Loadings

These diagnostic plots are designed to detect deviations from the model's implied relationship. In the example below, I'm going to utilize a simulated dataset of Jedi training, where two latent variables are posited (Force and Jedi), each with 3 indicators (fitness, saber, midichlorian for Force and exam one, exam two, and exam three for Jedi). In addition, one variable (history) has cross loadings on both factors. However, I will first fit a model where history is only modeled on the Force latent variable, then subsequently fit a model where the variable loads on both latent variables. 

\begin{center}
\begin{tikzpicture}[scale=1] 
\def\x{latex}

\draw (0,3) node[ellipse, minimum height=1cm,minimum width=1.5cm,draw] {Force};
\draw (7.5,3) node[ellipse, minimum height=1cm,minimum width=1.5cm,draw] {Jedi};
\draw (-2.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {fitness};
\draw (0,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {saber};
\draw (2.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {midichlorian};
\draw (5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {history};
\draw (7.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {exam one};
\draw (10,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {exam two};
\draw (12.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {exam three};

\draw[arrows={-\x}] (0,2.5) --(-2.5,.5) node[midway, above] {$a$};
\draw[arrows={-\x}] (0,2.5) --(0,.5) node[midway, left] {$b$};
\draw[arrows={-\x}] (0,2.5) --(2.5,.5) node[midway, above] {$c$};
\draw[arrows={-\x}] (0,2.5) --(5,.5) node[midway, above] {$d_1$};
\draw[arrows={-\x}] (7.5,2.5) --(5,.5) node[midway, above] {$d_2$};

\draw[arrows={-\x}] (7.5,2.5) --(7.5,.5) node[midway, right] {$e$};
\draw[arrows={-\x}] (7.5,2.5) --(10,.5) node[midway, right] {$f$};
\draw[arrows={-\x}] (7.5,2.5) --(12.5,.5) node[midway, above] {$g$};

\draw[arrows={-\x}] (.75,3) --(6.75,3) node[midway, above] {$\beta$};

\end{tikzpicture}
\end{center}

```{r, echo=FALSE, fig.cap="Scatterplot matrix of the indicator variables, but this time with local dependence between x1/x2 and a nonlinear relationship between the latent variable and x3. As before, the red line shows the implied fit between the two variables and the blue line shows the actual fitted relationship (measured via loess lines). \\label{fig:sixth}", fig.width=4, fig.height=3}
require(lavaan)
require(flexplot)
require(tidyverse)
data(jedi_jedi)


# specify the models ------------------------------------------------------
model = "
force_score =~ fitness + saber + midichlorian + force_history
jedi_score =~ exam_one + exam_two + exam_three
jedi_score ~ force_score
"
## specify model
model_cross = "
force_score =~ fitness + saber + midichlorian + force_history
jedi_score =~ exam_one + exam_two + exam_three + force_history
jedi_score ~ force_score
"
# Fit the models ----------------------------------------------------------
force_fit = cfa(model, jedi_jedi)
force_cross = cfa(model_cross, jedi_jedi)
```

The table below shows the result of both models and various measures of fit. If one were not to fit the second model, it would be unclear where the misfit occurs. One could, of course, study modification indices, but they have a multitude of problems. One could also study the residual matrix, but even these do not suggest how the model could be fixed. Rather, the data could be plotted, as they are in Figure \ref{fig:cross}. This makes it very apparent where the misfit comes from. 

```{r, echo=FALSE, message=FALSE, results='hide'}

sink("/dev/null");
fits_one = invisible(summary(force_fit, fit.measures=TRUE, header=FALSE, estimates=FALSE)[[1]])
fits_cross = summary(force_cross, fit.measures=TRUE, header=FALSE, estimates=FALSE)[[1]]
results = data.frame(cbind(fits_one, fits_cross)) %>% 
  mutate_all(round, digits=2) %>% 
  setNames(c("No Crossloadings", "Crossloadings Included"))
row.names(results) = names(fits_one)
sink()
```

```{r, echo=FALSE, message=FALSE, results="asis"}
knitr::kable(results)
```


```{r}
visualize(force_fit, force_cross, subset=4:7)
```

# Model Plots


Once one has iterated through the diagnostic plots, they can be more confident the factor scores are accurately estimated. At that point, the user may choose to plot the relationship between each indicator and the latent variable(s), or they may choose to plot the relationship between multiple latent variables. However, representing these as a scatterplot can be misleading because these are estimated scores, not observed scores. As such, I recommend adding some visual representation of uncertainty (e.g., each point is an ellipse whose size is an indicator of our uncertainty). This could, of course, become very busy very quickly. However, one could sample datapoints to represent latent scores or reduce opacity. These options are available and easy to use in my R package, `flexplot`. 

\pagebreak 

# References

## Stats Jedi Dataset

## Harry Potter Dataset

Suppose we have the following nonlinear relationship between observed variable $X$ and latent $F$: 

\begin{equation}
X = \frac{M}{1 + exp^{-x_1(F-x_0)}}
\label{eq:three}
\end{equation}

Solving for $F$, we get

\begin{equation}
F = \frac{-1}{x_1}log\left(\frac{M}{X}\right) -x_1
\label{eq:three}
\end{equation}

Let's assume the variable $Y$ is a linear function of $F$:

\begin{equation}
Y = y_0 + y_1 F
\label{eq:three}
\end{equation}

Solving for $F$, we get

\begin{equation}
F = \frac{Y-y_0}{y_1}
\label{eq:three}
\end{equation}

Now suppose we wish to estimate the value of $Y$ for a given $X$. To do so, we can use substitution:



## Scratch

\begin{center}
\begin{tikzpicture}[scale=1] 
\def\x{latex}
\draw (0,3) node[ellipse, minimum height=1cm,minimum width=1.5cm,draw] {Stats Jedi};
\draw (-2.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {exam 1};
\draw (0,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {exam 2};
\draw (2.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {exam 3};
\draw[arrows={-\x}] (0,2.5) --(-2.5,.5) node[midway, above] {$a$};
\draw[arrows={-\x}] (0,2.5) --(0,.5) node[midway, left] {$b$};
\draw[arrows={-\x}] (0,2.5) --(2.5,.5) node[midway, above] {$c$};
\end{tikzpicture}
\end{center}






\begin{center}
\begin{tikzpicture}[scale=1] 
\def\x{latex}

\draw (-1,5) node[ellipse, minimum height=1cm,minimum width=1.5cm,draw] (know) {Knowledge};
\draw (-1,0) node[ellipse, minimum height=1cm,minimum width=1.5cm,draw] (skills) {Skills};
\draw (-5,2.5) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] (surv) {Survival};

\draw (3.5,6.5) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {potions};
\draw (3.5,5) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {history};
\draw (3.5,3.5) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {herbology};

\draw[arrows={-\x}] (.33,5) --(2.75,6.5) node[midway, above] {};
\draw[arrows={-\x}] (.33,5) --(2.75,5) node[midway, above] {};
\draw[arrows={-\x}] (.33,5) --(2.65,3.4) node[midway, above] {};

\draw (3.5,1.5) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] (dark) {darkarts};
\draw (3.5,0) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {spells};
\draw (3.5,-1.5) node[rectangle, minimum height=1cm,minimum width=1.5cm,draw] {flying};

\draw (dark) edge[out=180,in=0,<-] (skills);
\draw[arrows={-\x}] (-.25,0) --(2.75,0) node[midway, above] {};
\draw[arrows={-\x}] (-.25,0) --(2.75,-1.5) node[midway, above] {};


\draw (know) edge[out=180,in=0,->] (surv);
\draw (skills) edge[out=180,in=0,->] (surv);

\end{tikzpicture}
\end{center}

<!-- magic_knowledge =~ potions + history + herbology -->
<!-- magic_skills =~ spells + darkarts + flying  -->
<!-- magic_skills ~ magic_knowledge -->
<!-- model = " -->
<!-- force_score =~ fitness + saber + midichlorian + force_history -->
<!-- jedi_score =~ exam_one + exam_two + exam_three -->
<!-- " -->



