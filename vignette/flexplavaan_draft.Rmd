---
title             : "Seeing the Impossible: Visualizing Latent Variable Models with flexplavaan"
shorttitle        : "Seeing the impossible"
    
author: 
  - name      : "Dustin Fife"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "201 Mullica Hill Road
                    Glassboro, NJ 08028"
    email         : "fife.dustin@gmail.com"
  - name      : "Steven Brunwasser"
    affiliation   : "1"  

affiliation:
  - id            : "1"
    institution   : "Rowan University"
documentclass     : "apa6"
classoption       : "doc"
output            : papaja::apa6_pdf
bibliography: flexplavaan_citations.bib 
header-includes:
- \usepackage{amsmath}
- \usepackage{tikz}
- \usetikzlibrary{shapes.geometric,arrows, positioning}
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE, note=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, note=FALSE, cache=TRUE)
require(tidyverse)
require(flexplavaan)
require(lavaan)
```

# Introduction
It is currently an unprecedented time in the social sciences; multiple scientific disciplines are reeling from a "replication crisis" [@Camerer2018; @Ioannidis2005; @Pashler2012a], new norms for credibility are becoming more prevalent (Nelson, Simmons, & Simonsohn, 2018; Nosek, Ebersole, DeHaven, & Mellor, 2018), and the push for open science is accelerating at a rapid pace [@Nosek2018]. Amidst this push for open science practices, some have called for greater use of visualization techniques [@Fife2019a; @Fife2019e; @Tay2016a]. As noted by Tay, et al. (2016), "[visualizations]...can strengthen the quality of research by further increasing the transparency of data..." (p. 694). In other words, one of the best, and most efficient ways of making data analysis open and transparent is to display each and every data point through visualization techniques. This is particularly important in research applications where participant-level data cannot be shared.

Not only do visualizations adhere to the principles of openness and transparency, but they offer several additional advantages; they vastly improve encoding of information [@Correll2015], they highlight model misfit [@Healy2014a], and they are an essential component in evaluating model assumptions [@Levine2018; @Tay2016a]. As such, we [as well as others, e.g., @Fife2019b; @Fife2019e; @Wilkinson1999a] recommend every statistical model ought to be accompanied by a graphic. 

Unfortunately, this suggestion is easier said than done. While visualizing some statistical models is trivial (e.g., regressions, $t$-tests, ANOVAs, multiple regression), visualizing others is not. One particularly troublesome class of models to visualize is latent variable models (LVMs). While researcher routinely visualize conceptual models (e.g., via path diagrams), visualizing the statistical models is not so easy. The former visualizations are common, while the latter are not [@Hallgren2019a]. The reason statistical visualizations of LVM are not intuitive is because they rely on unobserved variables [@bollen_structural_1989]. If the variables of interest are unobserved, how can we possibly visualize them? 

Though it is not, at first glance, easy to visualize unobserved variables, that does not mean visualizing them is any less important. On the contrary, visualizing latent variables is more important because their presence is unobserved. In the following section, we elaborate on how LVMs are traditionally evaluated and why visualizations are particularly crucial for LVMs. We then review previous approaches others have used for visualizing LVMs, and note their strengths and weaknesses. We then introduce our approach and the corresponding R package `flexplavaan`, which allows users to visualize both lavaan and blavaan objects in R. We then conclude with several examples that highlight how visualizations assisted in identifying appropriate statistical models. 

# Evaluating Model Fit in LVMs

The validity of LVM-based inferences assume structural models closely approximate real-world causal processes [@bollen2019model; @Hayduk2014]. Unfortunately, evaluating model adequacy in LVMs is rife with obstacles. For one, misspecifications in any one part of the model can lead to biases that spread throughout the full systems of equations [@bollen2019model]. To combat this, SEM practitioners generally rely on global fit tests and approximate fit indices to evaluate the model adequacy [@jackson2009reporting]. 

Yet global fit indices themselves represent an obstacle to intelligent model evaluation. Models can yield desirable values (e.g., a non-significant $\chi^2$ test), indicating a strong *overall* model-data correspondence, even when specific aspects of the model are misspecified [@Goodboy2017; @Hayduk2014; @tomarken2003potential]. In other words, a nonsensical model can still yield estimates that lead one to believe in their own statistical models. Moreover, in our experience, applied users often lack an intuitive understanding of what global fit statistics tell them about their models. Does a TLI of 0.95 mean we have established a strong theoretical foundation for a model? If RMSEA dips below 0.05, should we consider our model statistically/clinically significant? 

While global fit measures may be useful in comparing models and/or identifying problematic model features, users instead rely on conventional cutoffs [e.g, @hu1998fit] to determine whether a model is "adequate," a practice that has received resounding criticism [@barrett2007structural; @Chen2008; @hayduk2014shame; @McIntosh2007]. This lack of understanding is evident when applied users express confusion about why global fit indices provide conflicting assessments of model fit [@lai2016problem]. 

A number of scholars have emphasized the importance of supplementing SEM global fit indices with local fit assessment, investigating the tenability of all specific model implications individually [@bollen2019model; @Goodboy2017; @hayduk2014seeing; @Thoemmes2018; @tomarken2003potential; @Tomarken2005]. Local fit evaluation procedures (e.g., inspection of residual correlation matrices [@bollen_structural_1989], confirmatory tetrad analysis [@hipp2003model], and equation-based overidentification tests [@bollen2019model]) can help identify individual model specifications that are inconsistent with the data and may give clues about effective remedial strategies [@bollen_structural_1989; @Goodboy2017; @tomarken2003potential; @Tomarken2005]. 

While local fit indices will certainly improve model evaluation, they too suffer from a number of problems. First, [steve...care to add some here]. 
 
Another problem shared by both global and local fit indices is that they are a highly *compressed* representations of both the data and the model. Many readers may be familiar with Anscombe's quartet (reproduced in Figure \@ref(fig:anscombe)). There is a many-to-one relationship between data and indices; very different types of data may yield identical fit indices. Some of these data patterns may be very poorly represented by the model. This problem is only exacerbated with LVMs simply because traditional algorithms compress the data at multiple levels (e.g., raw data are compressed into means/covariances, which are then compressed into model parameter estimates and/or indices that evaluate model fit). Put differently (and perhaps quite cynically), LVM is the process of compressing hundreds or thousands of datapoints into a handful (or less) of estimates that may or may not represent the data-generating process. When considered from this perspective, the most common practices of evaluating model fit seem primitive, at best. 

```{r, anscombe, fig.cap="A reproduction of Anscombe's quartet. Each plot has identical regression lines/correlations, even though the underlying data are vastly different.", fig.width=6, fig.height=3}
require(tidyverse)
require(flexplavaan)
xs = anscombe %>% 
  pivot_longer(x1:x4, names_to="group", values_to="x", names_pattern="x(.*)") %>% 
  select(group, x)
ys = anscombe %>% 
  pivot_longer(y1:y4, names_to="group", values_to="y", names_pattern="y(.*)") %>% 
  select(y)

d = cbind(xs, ys)
ggplot(d, aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, fullrange=T) +
  facet_grid(~group, ) +
  theme_bw() + 
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )
```


The best defense against this compression is to rely on modeling evaluation strategies that evaluate *un*compressed data. Perhaps the best (if not only) way to evaluate uncompressed data is through visualizations, particularly visuals that display raw data [@Fife2019e]. 

# Previous Approaches to Visualizing LVMs
Ironically, while visuals of statistical models are both intuitive and effective at highlighting model misfit, there is sparse literature describing visual strategies for evaluating the adequacy of SEMs, at least when compared to the extensive literature discussing the merits of global fit tests and indices [e.g., @barrett2007structural; @Chen2008; @hayduk2014shame; @hu1998fit; @McIntosh2007; @shi2020effect; @smith2001primer; @Steiger2007]. While reporting of numeric fit indices is ubiquitous in LVM applications [@jackson2009reporting], plots of data underlying LVMs are rare [@Hallgren2019a]. There are, however, some strategies for using visuals to evaluate the tenability of model assumptions, diagnose causal misspecifications, and select the best model from a group of competitors.  

A common visual approach for identifying model-data discrepancies is to plot the distribution of the residuals of the covariances/correlation matrix (e.g., using stem-and-leaf plots or histograms) [@bollen_structural_1989]. This can aid in identifying specific components of the data that the model struggles to capture [@bollen_structural_1989; @bollen1991observational], which might not be detected with global fit indices [@Goodboy2017; @tomarken2003potential; @Tomarken2005]. 

Unfortunately, these plots suffer from two major problems. First, the residuals in this case are themselves *compressed* estimates. As such, we might have a model that is poorly represented by linear correlation (e.g., if the data contain nonlinear relationships), but that problem would never be uncovered by studying residual plots. 

A second problem with plots of correlation/covariance residuals is they are extremely limited in the amounts of misspecification they might reveal. For example, suppose we model a latent variable with three indicators ($x_1$, $x_2$, and $x_3$), but the data-generating model actually has $x_3$ associated with $x_2/x_1$, but not an indicator of the latent variable (see Figure \@ref(fig:implied)). These two models will have the same implied variance/covariance matrix.[^covmat] In other words, a stem and leaf plot will not signal any problems, despite problems existing. 

xxxxx mention equivalent models. flexplavaan can handle equivalent models!! xxxxxxxxxxxx
xxxx how to tell if it matters? xxxxx
* do they matter in predictions?
* do they matter in interventions?
* do they matter in terms of utility?

[^covmat]: In the left model, the standardized relationship between $x_1/x_3$ is $a_1\times c_1$, while in the right model it is $r_2$. Likewise, the $x_1/x_2$ relationship is $a_1\times b_1$ in the left model, while it is $r_1$ in the right model. The LVM machinery will attempt to make $a_1\times c_1 = r(x_1, x_3)$, which is the same as setting $a_1\times c_1 = r_2$ (and it will set $a_1\times b_1$ to $r_1$). 


\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto,>=latex, thick, scale = 0.5]
\node[draw, circle] (f) {$F$};
\node[draw, rectangle] (x2) [below of=f] {$X_2$};
\node[draw, rectangle] (x3) [right of=x2, node distance = 1cm] {$X_3$};
\node[draw, rectangle] (x1) [left of=x2, node distance = 1cm] {$X_1$};
\draw[->, left] (f) to node {$a_1$} (x1);
\draw[->] (f) to node {$b_1$} (x2);
\draw[->, right] (f) to node {$c_1$} (x3);

\node[draw, circle] (f2) [right of=f, node distance = 3cm] {$F$};
\node[draw, rectangle] (x22) [below of=f2] {$X_2$};
\node[draw, rectangle] (x32) [right of=x22, node distance = 1cm] {$X_3$};
\node[draw, rectangle] (x12) [left of=x22, node distance = 1cm] {$X_1$};
\draw[->, left] (f2) to node {$a_2$} (x12);
\draw[->, left] (f2) to node {$b_2$} (x22);
\draw[<->] (x22) edge[out=270, in=270,<->, below] node {$r_1$} (x32)  ;
\draw[<->] (x12) edge[out=270, in=270,<->, below] node {$r_2$} (x32.south east)  ;
\end{tikzpicture}
\end{center}
\caption{The model on the left is the user-specified model, while the model on the right is the data-generating model. These two models make very different theoretical statements, but have the same implied correlation between the variables.}
\label{fig:implied}
\end{figure}


Rather than visualizing aggregates in LVMs, the raw data itself ought to be visualized. @bollen1991observational developed methods for calculating raw and standardized individual case residuals (ICRs), which represent the difference between observed and model-estimated case values for outcome variables [@bollen1991observational]. These ICRs are then plotted to help locate outlying and influential observations. @pek2011sensitivity demonstrated how diagnostic procedures commonly used in generalized linear models (e.g., Mahalanobis distance, generalized Cook’s D, and DFBETAs) can be applied to LVMs to detect influential cases with index plots. @flora2012old applied these diagnostic procedures and others specifically to factor analysis models, and @yuan2010fitting used visualizations of Mahalanobis distance metrics to identify high-leverage cases and outliers. Open-source R packages, including `faoutlier` [@chalmers2017package] and `influence.SEM` [@pastore2018package], have used these visualization procedures to show case influence on model fit (e.g., likelihood differences) and parameter estimations (e.g., generalized Cook’s D).

@Asparouhov2017 proposed a method for extending the diagnosticity of ICRs to detect specific structural misspecifications. They demonstrated that plots of estimated factor scores against observed predictor variables can be used to detect unspecified nonlinear effects of the predictor on the latent outcome. Furthermore, they used ICR scatterplots to detect violations of local independence in a congeneric latent factor model. Finally, they demonstrated in a latent factor model how plotting predicted values for a reflective indicator against the observed indicator values could aid in uncovering unmodeled heterogeneity that could be better captured using a mixture model. 

@raykov2014latent used visualizations of ICRs to aid in model selection in the context of latent growth curve modeling. When comparing linear and quadratic growth curve models for the same data, for example, they showed that a scatterplot of the ICRs for the quadratic model vs. ICRs for a linear model can help identify which model best minimizes model-data discrepancies. In the context of growth mixture modeling, @wang2005residual showed how visualization of empirical Bayes residuals (e.g., Q-Q and trajectory plots) can aid in determining the appropriate number of classes, an adequate shape of within-class growth trajectories, and missing confounders.  

In short, ICRs have been used to identify high influence/leverage datapoints, nonlinear effects, heterogeneity, and to compare models. While these are certainly a step in the right direction, existing approaches suffer from a few weaknesses. First, ICRs rely on factor score estimates. Individual latent factor scores cannot be uniquely determined [@grice2001computing; @rigdon2019factor; @steiger1996dispelling]. In cases where factors are highly indeterminate -- e.g., factors with few indicators only weakly predicted by the latent factor -- different factor score estimation methods can yield highly discrepant values, potentially even estimates that are negatively correlated [@grice2001computing]. Also, ICRs are computed under the assumption the model is correct. When the model is misspecified, it is unclear how these visual diagnostics will behave. It is possible, of course, that for misspecified models, ICRs will reveal that misfit. (In fact, we show later that, at least under some circumstances, this is ineed the case). A final limitation of existing approaches is that many of their visuals cross platforms; some tools were developed in R (e.g., `influence.SEM` and `faoutlier`), some in Mplus [e.g., @Asparouhov2017], and some approaches provided no software implementation (e.g., xxxxx).

In this paper, we introduce `flexplavaan`, a suite of visualization tools designed to visualize `lavaan` models in R. Before we introduce `flexplavaan` and its core functions, however, we explain the types of graphics used and the rationale behind them. 

# Our Approach (Linear LVMs)

## Example Data

To motivate our discussion/explanation of visualizing LVMs, we begin with a simulated dataset. Suppose the Jedi council is attempting to identify padawans who will make good jedi knights. To do so, they develop seven indicators (light saber score, fitness score, midichlorian levels, and a jedi history exam, as well as three written exams completed at the end of jedi training). Unbeknownst to the Jedi, these indicators measure two latent factors (Force Propensity and Jedi Expertise), according to the relationships specified in Figure \@ref(fig:force). Notice that one variable (history) has cross loadings on both factors. Also notice the path from Force to Jedi is represented by a curved *one*-headed arrow. This is to indicate there is a curvilinear relationship between the two variables. 


<br>

\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm, midway, scale=1.5, every node/.style={scale=1.5}]

\node[draw, circle] (force) {Force};
\node[draw, circle, node distance = 2cm] (jedi) [right of=force] {Jedi};

\node[draw, rectangle] (saber) [left of=force] {Saber};
\node[draw, rectangle, node distance = 1cm] (fit) [above of=saber] {Fitness};
\node[draw, rectangle, node distance = 1cm] (midi) [below of=saber] {Midichlorian};

\node[draw, rectangle] (hist) [below of=jedi] {History};

\node[draw, rectangle] (e2) [right of=jedi] {Exam II};
\node[draw, rectangle, node distance = 1cm] (e1) [above of=e2] {Exam I};
\node[draw, rectangle, node distance = 1cm] (e3) [below of=e2] {Exam III};

\draw[->] (force) to node [fill=white]{\scriptsize $b$} (saber);
\draw[->] (force) to node [fill=white]{\scriptsize $a$} (fit);
\draw[->] (force) to node [fill=white]{\scriptsize $c$} (midi);

\draw[->] (jedi) to node [fill=white]{\scriptsize $d$} (e1);
\draw[->] (jedi) to node [fill=white]{\scriptsize $e$} (e2);
\draw[->] (jedi) to node [fill=white]{\scriptsize $f$} (e3);

\draw[->] (jedi) to node [fill=white]{\scriptsize $g_1$} (hist);
\draw[->] (force) to node [fill=white]{\scriptsize $g_2$} (hist);

\draw[->] (force) edge[out=25, in=180,->, below] node {\scriptsize $\beta$} (jedi);
\end{tikzpicture}
\label{fig:force}
\caption{Simulated dataset with crossloadings on the 'history' indicator. This is the data-generating model.}
\end{center}
\end{figure}


Unfortunately, the Jedi council posits the model shown in Figure \@ref{fig:council_model}. (Jedi are notoriously poor at psychometrics). While most of the important elements are there, the Jedi's (incorrect model) model specifies that history loads only onto force, and they posit a linear (rather than nonlinear) path from Force to Jedi. According to traditional measures of fit, the $\chi^2$ is statistically significant, but CFI (0.951), TLI (0.921), RMSEA (0.046), and SRMR (0.037) indicate respectable fit. In the examples that follow, we will use this example both to demonstrate how the visualization algorithms function and how they are able to identify misspecification that traditional fit statistics fail to capture. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm, midway, scale=1.5, every node/.style={scale=1.5}]

\node[draw, circle] (force) {Force};
\node[draw, circle, node distance = 2cm] (jedi) [right of=force] {Jedi};

\node[draw, rectangle] (saber) [left of=force] {Saber};
\node[draw, rectangle, node distance = 1cm] (fit) [above of=saber] {Fitness};
\node[draw, rectangle, node distance = 1cm] (midi) [below of=saber] {Midichlorian};

\node[draw, rectangle] (hist) [below of=jedi] {History};

\node[draw, rectangle] (e2) [right of=jedi] {Exam II};
\node[draw, rectangle, node distance = 1cm] (e1) [above of=e2] {Exam I};
\node[draw, rectangle, node distance = 1cm] (e3) [below of=e2] {Exam III};

\draw[->] (force) to node [fill=white]{\scriptsize $b$} (saber);
\draw[->] (force) to node [fill=white]{\scriptsize $a$} (fit);
\draw[->] (force) to node [fill=white]{\scriptsize $c$} (midi);

\draw[->] (jedi) to node [fill=white]{\scriptsize $d$} (e1);
\draw[->] (jedi) to node [fill=white]{\scriptsize $e$} (e2);
\draw[->] (jedi) to node [fill=white]{\scriptsize $f$} (e3);

\draw[->] (force) to node [fill=white]{\scriptsize $g_2$} (hist);
\draw[dotted, ->] (jedi) to node [fill=white]{\scriptsize $g_1$} (hist);

\draw[->, line width = .7mm] (force) to node [above]{\scriptsize $\beta$} (jedi);
\end{tikzpicture}
\label{fig:force}
\caption{Simulated dataset with crossloadings on the 'history' indicator. This is the data-generating model.}
\end{center}
\end{figure}

## Hopper Plots

Traditionally, conscientious researchers wanting to engaged in model evaluation will often produce stem and leaf diagrams of the residual variance/covariance matrix. However, stem and leaf diagrams are somewhat less intuitive, especially for those without experience interpreting them. As an alternative, we suggest using what we call "hopper plots," which plot the size of the residuals against the rank-ordered correlations (see Figure \@ref(fig:hopper)). The dots represent the residual size, while the lines show the absolute value of the residual (on the right) and $-1\times$ the absolute value of the residual (on the left). These plots end up looking like a funnel, but the name "funnel plot" was already snagged by meta-analytic researchers. Instead, we call them hopper plots. (A hopper is a type of funnel, frequently used to dispense grains). By default, hopper plots only show those variables with residuals larger than 0.01 (in absolute value). 

```{r hopper, fig.cap='An example of a "Hopper Plot," which graphs the discrepancy between the implied and observed correlation matrix. The dots represent observed residuals, while the lines represent the absolute value/negative absolute value.'}
require(ggplot2)
residual_plots(force_fit) + theme(text=element_text(size=14))
```


## Diagnostic Plots: Trail Plots

In order to conceptualize our approach to visualizing LVMs, let us first consider how typical linear models are visualized. In a standard regression, each dot in a scatterplot represents scores on the observed variables. Often, analysts overlay additional symbols to represent the fit of the model (e.g., a line to represented the fitted regression model, or large dots to represent the mean). Sometimes additional symbols are overlaid to represent uncertainty (e.g., confidence bands for a regression line or standard error bars). See Figure \@ref(fig:scatter) as an example. In either case, the dots represent observed information, while the fitted information is conveyed using other symbols. 

```{r scatter, fig.cap="Example figure that shows how standard statistical models are visualized. Dots represent scores on observed variables, while other symbols (e.g., regression line, large dots) represent the fit of the model.", fig.width=7, fig.height = 4 }
a = flexplot::flexplot(weight.loss~therapy.type, data=exercise_data) + labs(x="Therapy", y="Weight Loss")
b = flexplot::flexplot(weight.loss~motivation, data=exercise_data) + labs(x="Motivation", y="Weight Loss")
cowplot::plot_grid(a,b)
```

Likewise, visualizing LVMs might follow similar conventions; the dots should represent the observed information, as in @Bauer2005. In his visuals, pairwise relationships between observed variables are represented in a scatterplot. However, Bauer’s approach did not overlay a model-implied fit, as we seek to do. When the line represents the model-implied fit, it denotes the "trail" left behind by the unobserved latent variable. As such, we call these plots "trail plots."

How then does one identify the slope/intercept of the LVM’s model-implied fit? It is quite easy to do so when standard linear LVMs are used. Recall how our Force Propensity factor has four indicators (e.g., saber, fitness, midichlorians, and history). To visualize the bivariate relationship between saber and fitness, for example, we can simply utilize the model-implied variance/covariance matrix. Recall the relationship between a covariance and a slope:

$$\beta_{y|x} = \frac{\sigma^2(x,y)}{\sigma^2_x}$$

For our example, 

$$b_{\text{S}|\text{F}} = \frac{\hat{\sigma}^2_{\text{S},\text{F}}}{\hat{\sigma}^2_F}$$   

where $S$ and $F$ represent "saber" and "fitness," respectively, and ${\hat{\sigma}^2_F}$ represents the *residual* variance of fitness. (Put differently, this is the expected slope between saber and fitness, unadjusted for unreliability). With the slope, one can then estimate the intercept using basic algebra:

$$b_0=\overline{S}-\beta_{\text{S}|\text{F}}\times \overline{\text{F}}$$

Figure \@ref(fig:trace1) shows the LVM model-implied fit in red with a regression line in blue for simulated data. Because the regression line minimizes the sum of squared errors, we would hope the LVM fitted line (red) closely approximate the regression line (blue). In this case, the two are similar, but there is a visually detectable difference between the two; the model (red line) underestimates the relationship between the two variables. In other words, the LVM fails to capture the entire relationship between the two observed variables. 

```{r trace1, fig.cap = "The LVM-implied fit between fitness score and light saber score, shown in red. The blue line represents the regression line between the two variables. The more closely the model-implied fit line resembles the regresson line, the better the fit of the LVM."}
  # compute the model-implied slope
viz_diagnostics(jedi_jedi, aes(y=fitness, x=saber), fit.lavaan = force_fit$lavaan, plot="trace", method="lm")+
  labs(y="Fitness Score", x="Light Saber Score") +
  theme_bw()+ theme(text=element_text(size=14))
```


Of course, Figure \@ref(fig:trace1) only shows one pairwise relationship between variables. If we wished to visualize all the variables in our model, we would have to utilize a scatterplot matrix, as in Figure \@ref(fig:traceMatrix). The diagonal elements show histograms of ICRs, enabling researchers to (somewhat) evaluate the assumption of normality.[^mvnorm]  Naturally, this becomes quite cumbersome when users have more than seven or eight variables. In this case, it is best to visualize only a subset of variables. We will later discuss strategies for how best to select appropriate subsets. For our figure (Figure \@ref(fig:traceMatrix)), we have examined only the variables associated with the latent Force variable, while Figure \@ref(fig:trace_two) shows the variables associated with the latent Jedi variable. We have also asked flexplavaan to show a loess line instead of a regression line, which will allow us to detect nonlinear patterns. These figures reveals some potential problems, including some nonlinearity, some underestimation (e.g., between saber and midichlorian), and some overestimation (e.g., between saber and force_history). 

[^mvnorm]: Technically, LVMs assume multivariate normality, while these plots show univariate normality. However, the univariate plots at least suggest when multivariate normality might be violated. 

```{r traceMatrix, fig.cap="Scatterplot matrix showing the model-implied fit (red) and regression-implied fit (blue) between three simulated indicator variables. The diagonals show the histograms."}
require(flexplavaan)
flexplavaan::visualize(force_fit, plot="model", subset=c("saber", "fitness", "midichlorian", "force_history"), sort_plots = FALSE)
```


The primary advantage of trail plots is that they easily show many types of misspecification in LVMs. Another advantage is they visually (and often times strikingly) show how little information a model might capture. Returning to Figure \@ref(fig:traceMatrix), we see that many of the bivariate plots reveal quite weak relationships; many of the slopes are quite near zero. Recall that global fit indices suggested a well-fitting model. The trace plots, however, suggest there's little information to fit from the beginning for at least some of these relationships.  

```{r traceMatrix2, fig.cap="Scatterplot matrix showing the model-implied fit (red) and regression-implied fit (blue) between three simulated indicator variables. The diagonals show the histograms."}
require(flexplavaan)
flexplavaan::visualize(force_fit, plot="model", subset=c("exam_one", "exam_two", "exam_three", "force_history"), sort_plots = FALSE)+ theme(text=element_text(size=14))
```

# Disturbance-Dependence Plots
	
One common technique for visualizing the adequacy of statistical models in classic regression is residual-dependence plots. With these graphics, one simply plots the residuals of the model ($Y$ axis) against the predicted values ($X$ axis). The rationale behind this is simple: the model should have extracted any association between the prediction and the outcome. The residuals represent the remaining information after extracting the signal from the model. If there is a clear trend remaining in the data (e.g., a nonlinear pattern or a "megaphone" shape in the residuals), this indicates the model failed to capture important information.
	
Likewise, in LVMs, we can apply this same idea to determine whether the fit implied by the LVM has successfully extracted any association between any pair of predictors. However, in LVMs, residuals refer to the discrepancy between the model-implied and the actual variance/covariance matrix (or correlation matrix). As such, naming these plots "residual-dependence plots" would be a misnomer. Rather, misfit at the raw data level is typically called either a disturbance or an individual case residual (ICR), as mentioned previously. In this paper, we call these plots disturbance-dependence plots. 

Like trace plots, we visualize disturbance-dependence plots for each pair of observed variables. To do so, `flexplavaan` subtracts the fit implied by the model from the observed scores. For example, a disturbance dependence plot for an $X_1/X_2$ relationship would subtract the "fit" of $X_2$ implied by the model from the actual $X_2$ scores (and vice versa for the $X_2/X_1$ relationship). If the trace-plot fit actually extracts all association between the pair of observed variables, we would expect to see a scatterplot that shows no remaining association between the two. If there is a pattern in the scatterplot remaining, we know the fit of the model misses information about that specific relationship. 

To aid in interpreting these plots, we can overlay the plot with a flat line (with a slope of zero), as well as a regression (or loess) line. The first line indicates what signal should remain after fitting the model, while the second line shows what actually remains. 

Figure \@ref(fig:ddp) shows an example of trace plots in the upper triangle and disturbance-dependence plots in the lower triangle of a scatterplot matrix. These plots are for the same data shown in the right image of Figure \@ref(fig:traceMatrix2). Notice how many of the plots have nonlinear patterns showing up in both the ddp and the trace plots. 

(ref:crossloadcitation) This plot shows a disturbance-dependence plot for the same data visualized in Figure \@ref(fig:traceMatrix2) in the lower triangle.

```{r ddp, fig.cap='(ref:crossloadcitation)'}
flexplavaan::visualize(force_fit, subset=c("exam_one", "exam_two", "exam_three", "force_history"), sort_plots = FALSE)+ theme(text=element_text(size=14))
```

Together, both of these plots (trace plots and diagnostic-dependence plots) serve as a critical diagnostic check. Both these plots will signal certain types of misfit both in the measurement and structural components of the model. However, these plots suffer from a major weakness. Recall how earlier we referenced Figure \@ref(fig:implied) and noted that sometimes severe misspecification will go undetected simply because an incorrect model will often yield a model-implied covariance matrix that well approximates the actual covariance matrix. However, this sort of misspecification may show up in measurement plots, which we address next. 

# Measurement Plots

Earlier we mentioned how @Asparouhov2017 utilized factor score estimates to visualize ICRs as a diagnostic tool. One of their plots showed the latent variable on the $Y$ axis and the observed (indicator) variable on the $X$ axis. While these may be capable of revealing nonlinearities, they cannot reveal other sorts of misspecification (e.g., many types of cross-loadings and residual correlations) without a simple modification. The modification we propose is similar to the traceplots: overlay the model-implied slope and a regression (or loess) line. 

Fortunately, similar to the trace plots, we can use the model-implied variance/covariance matrix of the observed/latent variables to determine the model-implied slope. The advantage of these plots is they are far more sensitive to many types of misspecification than trace plots. This is because trace plots are unable to pick up misspecification unless that misspecification introduces bias in estimating the observed variance/correlation matrix. However, as shown in Figure \@ref(fig:implied), not all misspecification manifests itself as bias in estimating correlations between observed variables. While the two models in Figure \@ref(fig:implied) will not yield different observed covariances, they will yield different latent variable estimates (and thus, different covariances between latent/observed). 

Figure \@ref(fig:measurementplot) plots several graphs of the relationship between the observed variables and the latent variables. To do so, `flexplavaan` puts all variables on a common scale. Additionally, `flexplavaan` defaults to displaying only four observed variables at a time. Which four are chosen is determined by the degree of discrepancy between the observed (blue) and model-implied (red) slope, such that the four observed variables with the largest discrepancy are chosen. From Figure \@ref(fig:measurementplot), we see that the model consistently underestimates the relationship between the indicators and the latent variables, and that this underestimation is stronger for the jedi_score latent variable. It is also interesting to note that the force_history indicator is nearly synonymous with the force_score latent variable. 

```{r measurementplot, fig.cap='This image, called a measurement plot, shows the relationship between the latent variables ($Y$ axis) and each standardized indicator. The blue lines are loess lines, while the red lines are the model-implied fits of the model.'}
a = implied_measurement(force_fit, latent="force_score", method="loess", alpha=.2)[[1]]
b = implied_measurement(force_fit, latent="jedi_score", method="loess", alpha=.2)[[1]]
cowplot::plot_grid(a, b)
```


# Structural (Cross-Hair) Plots
<!-- One of the primary purposes of the diagnostics is to determine whether one's conceptualization of the latent variables is to be believed. If the trace plots and disturbance dependence plots indicate the LVM is a good representation of the data, one can be more confident the latent variables are properly estimated. If this is the case, we can now make a step toward visualizing the latent variables themselves.  -->
<!-- Another alteration from a standard scatterplot is the use of vertical bars to indicate uncertainty in estimating the latent variables. In Figure \@ref(measurementplot), the dots represent the estimated factor scores and the lines indicate $\pm 1 SD$. The red line is a "ghost line" [@Fife2019c], which simply repeats the line from the second panel ($x2$) to the other panels. This makes it easier to identify which indicators best load onto the latent variable (i.e., which indicator is most reliable). In Figure \@ref(fig:measurementplot), $x2$ has the largest factor score.  -->

xxxx these are useless if you don't have the other plots (holmgren article) xxxxx
When modeling latent variables, often the visuals of interest are not the observed variables, but the latent variables. In other words, the measurement model is ancillary to the substantive model. Naturally, we might wish to visualize the relationship between the latent variables.

However, our latent variables are merely estimates. As such, we ought to have visuals that reflect uncertainty in our estimates of the latent scores. In `flexplavaan`, this uncertainty is represented as crosshairs. The widths of each line of the crosshair (for both the $X$ axis and the $Y$ axis) are obtained from prediction intervals for `lavaan` objects (using the `plausibleValues` function of the `semTools` package). Figure \@ref(fig:beech) shows these plots, which we call "Structural Plots," or "Cross-hair Plots". Interestingly, the relationship between the two, though simulated to be significantly nonlinear, shows up as fairly linear. This seems to suggest that, by the time a model's factor scores are estimated, any nonlinearity that exists in the data has been discarded.  

```{r beech, fig.cap='Structural or "Cross-Hair" plot of the relationship between the latent variables Force and Jedi. The crosshairs represent the prediction intervals for the factor scores of the latent variables.', echo=FALSE}
flexplavaan::visualize(force_fit, plot="latent", method="loess")[[1]]

```

There is a great deal of flexibility in how one visualizes the structural model. Flexplavaan makes a best guess at how to visualize this relationship using the model specified by the user. However, the user can always specify how to plot the structural model using a `flexplot` equation [@Fife2019c]. In our example, we only had two variables to visualize, so a simple bivariate plot was most natural. When more variables are included, we might utilize paneling, added variable plots, beeswarm plots, etc. For a review of the types of plots possible, see @Fife2019c. 

# Model Comparisons

```{r}
d = jedi_jedi %>% mutate(
  fit_exp = (scale(fitness))^2,
  saber_exp = (scale(saber))^2,
  midi_exp = (scale(midichlorian))^2,
  history_exp = (scale(force_history))^2
)
model = "
force_score =~ fitness + saber + midichlorian + force_history
force_exp =~ fit_exp + saber_exp + midi_exp + force_history
jedi_score =~ exam_one + exam_two + exam_three + force_history 
jedi_score ~ force_score + force_exp
force_score ~~ force_exp
"
force_exp = flexplavaan(model, d)
residual_plots(force_fit, force_exp)
```

```{r}
visualize(force_fit, force_exp, subset=c("saber", "fitness", "midichlorian", "force_history"))
```

```{r}
implied_measurement(force_fit, force_exp, latent="force_score", alpha = .5)
```

```{r}
visualize(force_fit, force_exp, plot="latent")
```



# References