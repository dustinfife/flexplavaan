---
title             : "Seeing the Impossible: Visualizing Latent Variable Models with `flexplavaan`"
shorttitle        : "Seeing the impossible"
    
author: 
  - name      : "Dustin A. Fife"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "201 Mullica Hill Road
                    Glassboro, NJ 08028"
    email         : "fife.dustin@gmail.com"
  - name      : "Steven Brunwasser"
    affiliation   : "1"
  - name          : "Edgar C. Merkle"
    affiliation   : "2"  

affiliation:
  - id            : "1"
    institution   : "Rowan University"
  - id            : "2"
    institution   : "University of Missouri"

abstract: 
  "Latent variable models (LVMs) are incredible flexible tools that allow users to address research questions they might otherwise never be able to answer [mcdonald2013test]. However, one major limitation of LVMs is evaluating model fit. There is no universal consensus about how to evaluate model fit, either globally or locally. Part of the reason evaluating these models is difficult is because fit is typically reduced to a handful of statistics that may or may not reflect the model's adequacy and/or assumptions. In this paper we argue that proper evaluation of model fit *must* include visualizing both the raw data and the model-implied fit. Visuals reveal, at a glance, the fit of the model and whether the model's assumptions have been met. Unfortunately, tools for visualizing LVMs have historically been limited. In this paper, we introduce new plots and reframe existing plots that provide necessary resources for evaluating LVMs. These plots are available in a new open-source R package called `flexplavaan`, which aims to work hand-in-hand with the R package `lavaan`."
documentclass     : "apa6"
classoption       : "doc"
output            : papaja::apa6_pdf
bibliography: 
- flexplavaan_citations.bib 
- references.bib 
header-includes:
- \usepackage{amsmath}
- \usepackage{tikz}
- \usetikzlibrary{shapes.geometric,arrows, positioning}
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE, note=FALSE}
knitr::opts_chunk$set(echo = FALSE, message=FALSE, warning=FALSE, note=FALSE, cache=TRUE)
require(tidyverse)
require(flexplavaan)
require(lavaan)
```
 
# Introduction
It is currently an unprecedented time in the social sciences; multiple scientific disciplines are reeling from a "replication crisis" [@Camerer2018; @Ioannidis2005; @Pashler2012a], new norms for credibility are becoming more prevalent (Nelson, Simmons, & Simonsohn, 2018; Nosek, Ebersole, DeHaven, & Mellor, 2018), and the push for open science is accelerating at a rapid pace [@Nosek2018]. Amidst this push for open science practices, some have called for greater use of visualization techniques [@Fife2019a; @Fife2019e; @Tay2016a]. As noted by Tay, et al. (2016), "[visualizations]...can strengthen the quality of research by further increasing the transparency of data..." (p. 694). In other words, one of the best, and most efficient ways of making data analysis open and transparent is to display each and every data point through visualization techniques. This is particularly important in research applications where participant-level data cannot be shared.

Not only do visualizations adhere to the principles of openness and transparency, but they offer several additional advantages; they vastly improve encoding of information [@Correll2015], they highlight model misfit [@Healy2014a], and they are an essential component in evaluating model assumptions [@Levine2018; @Tay2016a]. As such, we [as well as others, e.g., @Fife2019b; @Fife2019e; @Wilkinson1999a] recommend every statistical model ought to be accompanied by a graphic. 

Unfortunately, this suggestion is easier said than done. While visualizing some statistical models is trivial (e.g., regressions, $t$-tests, ANOVAs, multiple regression), visualizing others is not. One particularly troublesome class of models to visualize is latent variable models (LVMs). While researchers routinely visualize LVMs with conceptual models (e.g., via path diagrams), visualizing the fits of statistical models is not so easy. The former visualizations are common, while the latter are not [@Hallgren2019a]. The reason statistical visualizations of LVM are not intuitive is because they rely on unobserved variables [@bollen_structural_1989]. If the variables of interest are unobserved, how can we possibly visualize them? 

Though it is not, at first glance, intuitive how to visualize unobserved variables, that does not mean visualizing them is any less important. On the contrary, visualizing latent variables is more important because their presence is unobserved. In the following section, we elaborate on how LVMs are traditionally evaluated and why visualizations are particularly crucial. We then review previous approaches others have used for visualizing LVMs, and note their strengths and weaknesses. We then introduce our approach to visualizing LVMs. Our strategy approaches visualization differently from existing approaches and is able to detect misfit in ways undetected by traditional fit indices and visualization strategies. Furthermore, our approach allows users to detect misfit in just-identified models, as well as differences in fit for equivalent models. This approach and the corresponding R package `flexplavaan`, which allows users to visualize both `lavaan` and `blavaan` objects in R. We then conclude with examples that highlight how visualizations assisted in identifying problematic statistical models. 

# Evaluating Model Fit in LVMs

The validity of LVM-based inferences assume structural models closely approximate real-world causal processes [@bollen2019model; @Hayduk2014]. Unfortunately, evaluating model adequacy in LVMs is rife with obstacles. For one, misspecifications in any one part of the model can lead to biases that spread throughout the full systems of equations [@bollen2019model]. To address this issue, LVM practitioners generally rely on global fit tests and approximate fit indices to evaluate the model adequacy [@jackson2009reporting]. 

Yet global fit indices themselves represent an obstacle to intelligent model evaluation. Models can yield desirable values (e.g., a non-significant $\chi^2$ test), even when specific aspects of the model are substantially misspecified [@Goodboy2017; @Hayduk2014; @tomarken2003potential]. In other words, a nonsensical model can still yield estimates that lead one to believe in their own statistical models. Moreover, in our experience, applied users often lack an intuitive understanding of what global fit statistics tell them about their models. Does a Tucker-Lewis Index (TLI) value of 0.95 mean we have established a strong theoretical foundation for a model? If the root mean square error of approximation (RMSEA) value dips below 0.05, should we assumed that the model estimates closely approximate real-world relationships? This lack of understanding is evident when applied users express confusion about why global fit indices sometimes provide seemingly conflicting assessments of model fit [@lai2016problem]. 

While global fit measures may be useful in comparing models and/or identifying problematic model features, users instead rely on conventional cutoffs [e.g, @hu1998fit] to determine whether a model is "adequate," a practice that has received resounding criticism [@barrett2007structural; @Chen2008; @hayduk2014shame; @McIntosh2007]. 

A number of scholars have emphasized the importance of supplementing LVM global fit indices with local fit assessment, investigating the tenability of all specific model implications individually [@bollen2019model; @Goodboy2017; @Hayduk2014; @Thoemmes2018; @thoemmes2018local; @tomarken2003potential; @Tomarken2005]. Local fit evaluation procedures (e.g., inspection of residual correlation matrices [e.g., @bollen_structural_1989], confirmatory tetrad analysis [@hipp2003model], and equation-based overidentification tests [@bollen2019model]) can help identify individual model specifications that are inconsistent with the data and may give clues about effective remedial strategies [@bollen_structural_1989; @Goodboy2017; @thoemmes2018local; @tomarken2003potential; @Tomarken2005]. 

While local fit indices are an important part of model evaluation @Kline2004, they too have limitations. First, although local fit evaluation can be helpful in identifying which implied bivariate associations are most discrepant with the data, they do not necessarily pinpoint the source of the problems [@hayduk2018review]. For instance, an error in the specification of the measurement of an exogenous latent variable could lead to a large residual correlation between measured indicators for the exogenous and endogenous latent variables even if the structural portion of the model is properly specified. Notably, the model-implied instrumental variable (MIIV) approach can contain problems caused by misfit in one aspect of a model and prevent ill effects from spreading [@bollen2019model]. Second, it is unclear when a non-zero residual correlation should be interpreted as meaningful. Imposing a cutoff value is one approach @Goodboy2017, though the magnitude of discrepancies is an imperfect metric for determining its practical import [@Hayduk2014]. Standardized residual correlations allow for significance tests for each element of the residual correlation matrix, though residual correlation matrices often include many elements, requiring an adjustment to the p-value to avoid an inflated Type I error rate [@bollen1991observational]. Additionally, residual correlations may not alert modelers to strong nonlinear associations among observed variables [@flora2012old]. Finally, local fit indices often produce an overwhelming amount of statistical information that must be filtered. This can be a daunting process, particularly for those not accustomed to LVMs.  

[^1]: Notably, the model-implied instrumental variable (MIIV) approach can contain problems caused by misfit in one aspect of a model and prevent ill effects from spreading [@bollen2019model]. 

Another problem shared by both global and local fit indices is that they are a highly *compressed* representations of both the data and the model. Many readers may be familiar with Anscombe's quartet (reproduced in Figure \@ref(fig:anscombe)). Anscombe demonstrated there are *man* different raw data patterns that can yield identical correlations/slopes/intercepts. For this reason, simply reporting statistics without visualizing them can be misleading. Likewise, with LVMs, there is a many-to-one relationship between data and indices; very different types of data may yield identical statistics (in this case, correlations, slopes and intercepts). Some of these data patterns may be very poorly represented by the model. This problem is only exacerbated with LVMs simply because traditional algorithms compress the data at multiple levels (e.g., raw data are compressed into means/covariances, which are then compressed into model parameter estimates and/or indices that evaluate model fit). Put differently (and perhaps quite cynically), LVM is the process of compressing hundreds or thousands of data points into a handful (or less) of estimates that may or may not represent the data-generating process. When considered from this perspective, the most common practices of evaluating model fit seem primitive, at best. 

```{r anscombe, fig.cap="A reproduction of Anscombe's quartet. Each plot has identical regression lines/correlations, even though the underlying data are vastly different.", fig.width=6, fig.height=3}
require(tidyverse)
require(flexplavaan)
xs = anscombe %>% 
  pivot_longer(x1:x4, names_to="group", values_to="x", names_pattern="x(.*)") %>% 
  select(group, x)
ys = anscombe %>% 
  pivot_longer(y1:y4, names_to="group", values_to="y", names_pattern="y(.*)") %>% 
  select(y)

d = cbind(xs, ys)
ggplot(d, aes(x=x, y=y)) +
  geom_point() +
  geom_smooth(method="lm", se=FALSE, fullrange=T) +
  facet_grid(~group, ) +
  theme_bw() + 
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )
```


The best defense against this compression is to rely on modeling evaluation strategies that evaluate *un*compressed data. Perhaps the best way to evaluate uncompressed data is through visualizations, particularly visuals that display raw data [@Fife2019e]. 

# Previous Approaches to Visualizing LVMs
There is sparse literature describing visual strategies for evaluating the adequacy of LVMs, at least when compared to the extensive literature discussing the merits of global fit tests and indices [e.g., @barrett2007structural; @Chen2008; @hayduk2014shame; @hu1998fit; @McIntosh2007; @shi2020effect; @smith2001primer; @Steiger2007]. Additionally, while reporting of numeric fit indices is ubiquitous in LVM applications [@jackson2009reporting], plots of data underlying LVMs are rare [@Hallgren2019a]. There are, however, some strategies for using visuals to evaluate the tenability of model assumptions, diagnose causal misspecifications, and select the best model from a group of competitors.  

A common visual approach for identifying model-data discrepancies is to plot the distribution of the residuals of the covariances/correlation matrix [e.g., using stem-and-leaf plots or histograms; @bollen_structural_1989]. This can aid in identifying specific components of the data that the model struggles to capture [@bollen_structural_1989; @bollen1991observational], which might not be detected with global fit indices [@Goodboy2017; @tomarken2003potential; @Tomarken2005]. Unfortunately, these plots suffer from two major problems. First, the residuals in this case are themselves compressed estimates. As such, we might have a model that is poorly represented by linear correlations (e.g., if the data contain nonlinear relationships), but that problem would never be uncovered by studying residual plots. 

A second problem with plots of correlation/covariance residuals is they are extremely limited in the amounts of misspecification they might reveal. For example, suppose we posit a latent variable with three indicators: $X_1$, $X_2$, and $X_3$ (see left image in Figure \@ref(fig:implied)). However, the data-generating model actually has $X_3$ associated with $X_2/X_1$ (right diagram in Figure \@ref(fig:implied)).[^equiv] These two models will have the same implied variance/covariance matrix.[^covmat] In other words, a stem and leaf plot will not signal any problems, despite problems existing. 

[^equiv]: This problem is similar, but not identical to equivalent models [@Lee90; @MacCallum93]. When two models are equivalent, they have identical variance/covariance matrices *and* identical degrees of freedom (DF). In this case, the two models do not have identical DF. However, the visuals we present do offer an intuitive approach for evaluating equivalent models. Two equivalent models with different latent models will have different visuals. Additionally, these visual will allow one to detect problems with equivalent models, which cannot be detected with traditional model-evaluation statistics. 


[^covmat]: The model on the left is the user-specified model. The model on the right is the data-generating model. In the left model, the standardized relationship between $X_1/X_3$ is $a_1\times c_1$, while in the right model it is $r_2$. Likewise, the $X_1/X_2$ relationship is $a_1\times b_1$ in the left model, while it is $r_1$ in the right model. The LVM machinery will attempt to make $a_1\times c_1 = r(X_1, X_3)$, which is the same as setting $a_1\times c_1 = r_2$ (and it will set $a_1\times b_1$ to $r_1$). In other words, the implied variance/covariance matrix will not reveal any problems, despite having a misspecified model. 


\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm, auto,>=latex, thick, scale = 0.5]
\node[draw, circle] (f) {$F$};
\node[draw, rectangle] (x2) [below of=f] {$X_2$};
\node[draw, rectangle] (x3) [right of=x2, node distance = 1cm] {$X_3$};
\node[draw, rectangle] (x1) [left of=x2, node distance = 1cm] {$X_1$};
\draw[->, left] (f) to node {$a_1$} (x1);
\draw[->] (f) to node {$b_1$} (x2);
\draw[->, right] (f) to node {$c_1$} (x3);

\node[draw, circle] (f2) [right of=f, node distance = 3cm] {$F$};
\node[draw, rectangle] (x22) [below of=f2] {$X_2$};
\node[draw, rectangle] (x32) [right of=x22, node distance = 1cm] {$X_3$};
\node[draw, rectangle] (x12) [left of=x22, node distance = 1cm] {$X_1$};
\draw[->, left] (f2) to node {$a_2$} (x12);
\draw[->, right] (f2) to node {$b_2$} (x22);
\draw[<->] (x22) edge[out=270, in=270,<->, below] node {$r_1$} (x32)  ;
\draw[<->] (x12) edge[out=270, in=270,<->, below] node {$r_2$} (x32.south east)  ;
\end{tikzpicture}
\caption{The model on the left is the user-specified model, while the model on the right is the data-generating model. These two models make very different theoretical statements, but have the same implied correlation between the variables. Visualizing the raw data from these models can help identify structural problems for the model on the left.}
\label{fig:implied}
\end{center}
\end{figure}


Rather than visualizing aggregates in LVMs, the raw data themselves ought to be visualized. @bollen1991observational developed methods for calculating raw and standardized individual case residuals (ICRs), which represent the difference between observed and model-estimated case values for outcome variables [@bollen1991observational]. These ICRs are then plotted to help locate outlying and influential observations. @pek2011sensitivity demonstrated how diagnostic procedures commonly used in generalized linear models (e.g., Mahalanobis distance, generalized Cook’s D, and DFBETAs) can be applied to LVMs to detect influential cases with index plots. @flora2012old applied these diagnostic procedures and others specifically to factor analysis models, and @yuan2010fitting used visualizations of Mahalanobis distance metrics to identify high-leverage cases and outliers. Open-source R packages, including `faoutlier` [@faoutlier] and `influence.SEM` [@influenceSEM], have used these visualization procedures to show case influence on model fit (e.g., likelihood differences) and parameter estimations (e.g., generalized Cook’s D). @lee2015parameter also demonstrated how visualization can be used to identify specific parameters estimates with strong influence on global model fit. 

@asparouhov2014using proposed a method for extending the diagnosticity of ICRs to detect specific structural misspecifications. They demonstrated that plots of estimated factor scores against observed predictor variables can be used to detect unspecified nonlinear effects of the predictor on the latent outcome. Furthermore, they used ICR scatterplots to detect violations of local independence in a congeneric latent factor model. Finally, they demonstrated in a latent factor model how plotting predicted values for a reflective indicator against the observed indicator values could aid in uncovering unmodeled heterogeneity that could be better captured using a mixture model. 

@raykov2014latent used visualizations of ICRs to aid in model selection in the context of latent growth curve modeling. When comparing linear and quadratic growth curve models for the same data, for example, they showed that a scatterplot of the ICRs for the quadratic model vs. ICRs for a linear model can help identify which model best minimizes model-data discrepancies. In the context of growth mixture modeling, @wang2005residual showed how visualization of empirical Bayes residuals (e.g., Q-Q and trajectory plots) can aid in determining the appropriate number of classes, an adequate shape of within-class growth trajectories, and missing confounders.  

In short, ICRs have been used to identify high influence/leverage data points, nonlinear effects, heterogeneity, and to compare models. While these are certainly a step in the right direction, existing approaches suffer from a few weaknesses. First, ICRs rely on factor score estimates. Individual latent factor scores cannot be uniquely determined [@grice2001computing; @rigdon2019factor; @steiger1996dispelling]. In cases where factors are highly indeterminate -- e.g., factors with few indicators only weakly predicted by the latent factor -- different factor score estimation methods can yield highly discrepant values, potentially even estimates that are negatively correlated [@grice2001computing]. Also, ICRs are computed under the assumption the model is correct. When the model is misspecified, it is unclear how these visual diagnostics will behave. It is possible, of course, that for misspecified models, ICRs will reveal that misfit. (In fact, we show later that, at least under some circumstances, this is indeed the case). A final limitation of existing approaches is that many of their visuals cross platforms [e.g., @asparouhov2014using; @faoutlier] and some were achieved using coding without a dedicated package [@raykov2014latent].

In this paper, we introduce `flexplavaan`, a suite of visualization tools designed to visualize `lavaan` models in R. Before we introduce `flexplavaan` and its core functions, however, we explain the types of graphics used and the rationale behind them. 

# Our Approach (Linear LVMs)

Our approach will introduce five different plots: hopper plots for visualizing residual correlations, trail plots and disturbance-dependence plots for visualizing model-implied fit, and measurement plots/structural plots for visualizing relationships with/between latent variables. This plots are adept at identifying various types of misfit and model assumption violations, such as nonlinearity, correlated errors, missing paths, outliers, nonnormality, etc. They are also capable of evaluating models that existing numeric approaches cannot evaluate (e.g., equivalent models and just-identified models). Before we introduce these approaches, we introduce our simulated example dataset. 

## Example Data

To motivate our discussion/explanation of visualizing LVMs, we begin with a simulated dataset. Suppose the Jedi Council is attempting to identify Padawans who will make good Jedi Knights. To do so, they develop seven indicators (light saber score, fitness score, midichlorian levels, and a Jedi history exam, as well as three written exams completed at the end of Jedi training). Unbeknownst to the Jedi, these indicators measure two latent factors (force and Jedi), according to the relationships specified in Figure \@ref(fig:force). Notice that one variable (history) has cross loadings on both factors. Also notice the path from force to Jedi is represented by a curved *one*-headed arrow. This is to indicate there is a curvilinear relationship between the two variables. 


<br>

\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm, midway, scale=1.5, every node/.style={scale=1.5}]

\node[draw, circle] (force) {Force};
\node[draw, circle, node distance = 2cm] (jedi) [right of=force] {Jedi};

\node[draw, rectangle] (saber) [left of=force] {Saber};
\node[draw, rectangle, node distance = 1cm] (fit) [above of=saber] {Fitness};
\node[draw, rectangle, node distance = 1cm] (midi) [below of=saber] {Midichlorian};

\node[draw, rectangle] (hist) [below of=jedi] {History};

\node[draw, rectangle] (e2) [right of=jedi] {Exam II};
\node[draw, rectangle, node distance = 1cm] (e1) [above of=e2] {Exam I};
\node[draw, rectangle, node distance = 1cm] (e3) [below of=e2] {Exam III};

\draw[->] (force) to node [fill=white]{\scriptsize $b$} (saber);
\draw[->] (force) to node [fill=white]{\scriptsize $a$} (fit);
\draw[->] (force) to node [fill=white]{\scriptsize $c$} (midi);

\draw[->] (jedi) to node [fill=white]{\scriptsize $d$} (e1);
\draw[->] (jedi) to node [fill=white]{\scriptsize $e$} (e2);
\draw[->] (jedi) to node [fill=white]{\scriptsize $f$} (e3);

\draw[->] (jedi) to node [fill=white]{\scriptsize $g_1$} (hist);
\draw[->] (force) to node [fill=white]{\scriptsize $g_2$} (hist);

\draw[->] (force) edge[out=25, in=180,->, below] node {\scriptsize $\beta$} (jedi);
\end{tikzpicture}
\caption{Simulated dataset of Jedi selection and training. This model features a crossloadings on the history indicator, as well as a nonlinear relationship between force and Jedi. This is the data-generating model.}
\label{fig:force}
\end{center}
\end{figure}

```{r}

fit = fitMeasures(force_fit$lavaan) %>% round(digits=4)
significance = ifelse(fit[["pvalue"]]>0.05, " is not statistically significant", " is statistically significant")
```

Unfortunately, the Jedi Council posits the model shown in Figure \@ref(fig:councilModel). (Jedi are notoriously poor at psychometrics.) While most of the important elements are there, the Jedi's (incorrect) model specifies that history loads only onto force, and they posit a linear (rather than nonlinear) path from force to Jedi. According to conventional measures of fit, the $\chi^2$ `r significance`. Also, the CFI (`r fit[["cfi"]]`), TLI (`r fit[["tli"]]`), RMSEA (`r fit[["rmsea"]]`), and SRMR (`r fit[["srmr"]]`) indicate respectable fit. In the examples that follow, we will use this example both to demonstrate how the visualization algorithms function and how they are able to identify misspecification that traditional fit statistics fail to capture. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm, midway, scale=1.5, every node/.style={scale=1.5}]

\node[draw, circle] (force) {Force};
\node[draw, circle, node distance = 2cm] (jedi) [right of=force] {Jedi};

\node[draw, rectangle] (saber) [left of=force] {Saber};
\node[draw, rectangle, node distance = 1cm] (fit) [above of=saber] {Fitness};
\node[draw, rectangle, node distance = 1cm] (midi) [below of=saber] {Midichlorian};

\node[draw, rectangle] (hist) [below of=jedi] {History};

\node[draw, rectangle] (e2) [right of=jedi] {Exam II};
\node[draw, rectangle, node distance = 1cm] (e1) [above of=e2] {Exam I};
\node[draw, rectangle, node distance = 1cm] (e3) [below of=e2] {Exam III};

\draw[->] (force) to node [fill=white]{\scriptsize $b$} (saber);
\draw[->] (force) to node [fill=white]{\scriptsize $a$} (fit);
\draw[->] (force) to node [fill=white]{\scriptsize $c$} (midi);

\draw[->] (jedi) to node [fill=white]{\scriptsize $d$} (e1);
\draw[->] (jedi) to node [fill=white]{\scriptsize $e$} (e2);
\draw[->] (jedi) to node [fill=white]{\scriptsize $f$} (e3);

\draw[->] (force) to node [fill=white]{\scriptsize $g_2$} (hist);
\draw[dotted, ->] (jedi) to node [fill=white]{\scriptsize $g_1$} (hist);

\draw[->, line width = .7mm] (force) to node [above]{\scriptsize $\beta$} (jedi);
\end{tikzpicture}
\caption{Actual model fitted to the Jedi dataset. This posits a linear (rather than nonlinear) relationship between the latent variables Jedi and force and fails to model the Jedi to history relationship.}
\label{fig:councilModel}
\end{center}
\end{figure}

## Hopper Plots

Traditionally, conscientious researchers wanting to engage in model evaluation will often produce stem-and-leaf diagrams of the residual variance/covariance matrix. However, stem-and-leaf diagrams are somewhat less intuitive, especially for those without experience interpreting them. As an alternative, we suggest using what we call "hopper plots," which plot the size of the residuals against the rank-ordered correlations (see Figure \@ref(fig:hopper)). In other words, as we go from top to bottom, the residual correlations will get smaller and smaller. The dots represent the residual size, while the lines show the absolute value of the residual (on the right) and $-1\times$ the absolute value of the residual (on the left). These plots end up looking like a funnel, but the name "funnel plot" was already snagged by meta-analytic researchers [@egger1997bias]. Instead, we call them hopper plots. (A hopper is a type of funnel, frequently used to dispense grains.) By default, hopper plots only show those variables with residuals larger than 0.01 (in absolute value). Ideally, the width of the top of the hopper will have a small residual value. In the plot shown in Figure \@ref(fig:hopper), we see that there are problems with fitting correlations with the variables saber, fitness, and midichlorian. 


```{r hopper, fig.cap='An example of a "hopper plot," which graphs the discrepancy between the implied and observed correlation matrix. The dots represent observed residuals, while the lines represent the absolute value/negative absolute value.'}
require(ggplot2)
residual_plots(force_fit) + theme(text=element_text(size=14))
```


## Trail Plots

In order to conceptualize our approach to visualizing LVMs, let us first consider how typical linear models are visualized. In a standard regression, each dot in a scatterplot represents scores on the observed variables. Often, analysts overlay additional symbols to represent the fit of the model (e.g., a line to represent the fitted regression model, or large dots to represent the mean). Sometimes additional symbols are overlaid to represent uncertainty (e.g., confidence bands for a regression line or standard error bars). In either case, the dots represent observed information, while the fitted information is conveyed using other symbols. 

Likewise, visualizing LVMs might follow similar conventions; the dots should represent the observed information, as in @Bauer2005. In his visuals, pairwise relationships between observed variables are represented in a scatterplot. However, Bauer’s approach did not overlay a model-implied fit, as we seek to do. When the line represents the model-implied fit, it denotes the "trail" left behind by the unobserved latent variable. As such, we call these plots "trail plots." Importantly, which variable falls on the $X$ versus $Y$ axis is arbitrary. 

How then does one identify the slope/intercept of the LVM’s model-implied fit? It is quite easy to do so when standard linear LVMs are used. Recall how our force factor has four indicators (e.g., saber, fitness, midichlorians, and history). To visualize the bivariate relationship between saber and fitness, for example, we can simply utilize the model-implied variance/covariance matrix. Recall the relationship between a covariance and a slope:

$$\beta_{y|x} = \frac{\sigma(x,y)}{\sigma^2_x}$$

For our example, 

$$b_{\text{S}|\text{F}} = \frac{\hat{\sigma}_{\text{S},\text{F}}}{\hat{\sigma}^2_F}$$   

\noindent where $S$ and $F$ represent "saber" and "fitness," respectively, and ${\hat{\sigma}^2_F}$ represents the *residual* variance of fitness. (Put differently, this is the expected slope between saber and fitness, unadjusted for unreliability). With the slope, one can then estimate the intercept using basic algebra:

$$b_0=\overline{S}-\beta_{\text{S}|\text{F}}\times \overline{\text{F}}$$

Figure \@ref(fig:trail1) shows the LVM model-implied fit in red with a regression line in blue. Because the regression line minimizes the sum of squared errors, we would hope the LVM fitted line (red) closely approximates the regression line (blue). In this case, the two are similar, but there is a visually detectable difference between the two; the model (red line) underestimates the relationship between the two variables. 

```{r trail1, fig.cap = "The LVM-implied fit between fitness score and light saber score, shown in red. The blue line represents the regression line between the two variables. The more closely the model-implied fit line resembles the regression line, the better the fit of the LVM."}
  # compute the model-implied slope
viz_diagnostics(jedi_jedi, aes(y=fitness, x=saber), fit.lavaan = force_fit$lavaan, plot="trace", method="lm")+
  labs(y="Fitness Score", x="Light Saber Score") +
  theme_bw()+ theme(text=element_text(size=14))
```


Of course, Figure \@ref(fig:trail1) only shows one pairwise relationship between variables. If we wished to visualize all the variables in our model, we would have to utilize a scatterplot matrix, as in Figure \@ref(fig:trailMatrix). The diagonal elements show histograms of ICRs, enabling researchers to (somewhat) evaluate the assumption of normality.[^mvnorm]  Naturally, this becomes quite cumbersome when users have more than seven or eight variables. In this case, it is best to visualize only a subset of variables. By default, `flexplavaan` sorts the scatterplot matrix using a blockmodeling algorithm, then presents the block with the largest residuals first. For our figure (Figure \@ref(fig:trailMatrix)), we have examined only the variables associated with the latent force variable, while Figure \@ref(fig:trailMatrix2) shows the variables associated with the latent Jedi variable. We have also asked `flexplavaan` to show a loess line instead of a regression line, which will allow us to detect nonlinear patterns. These figures reveals some potential problems, including some nonlinearity, some underestimation (e.g., between saber and midichlorian), and some overestimation (e.g., between saber and history). 

[^mvnorm]: Technically, maximum likelihood-based LVMs assume multivariate normality, while these plots show univariate normality. However, the univariate plots at least suggest when multivariate normality might be violated. 

```{r trailMatrix, fig.cap="Scatterplot matrix showing the model-implied fit (red) and loess lines (blue) between four simulated indicator variables. These four variables are the indicators for the force latent variable. The diagonals show the histograms of the ICRs."}
require(flexplavaan)
flexplavaan::visualize(force_fit, plot="model", subset=c("saber", "fitness", "midichlorian", "force_history"), sort_plots = FALSE)
```


The primary advantage of trail plots is that they easily show many types of misspecification in LVMs. Another advantage is they visually (and often times strikingly) show how little information a model might capture. Returning to Figure \@ref(fig:trailMatrix), we see that many of the bivariate plots reveal quite weak relationships; many of the slopes are quite near zero. Recall that global fit indices suggested a well-fitting model. The trail plots, however, suggest there's little information to fit from the beginning for at least some of these relationships.  

```{r trailMatrix2, fig.cap="Scatterplot matrix showing the model-implied fit (red) and regression-implied fit (blue) between four simulated indicator variables. These four variables are the indicators for the Jedi latent variable. The diagonals show the histograms."}
require(flexplavaan)
flexplavaan::visualize(force_fit, plot="model", subset=c("exam_one", "exam_two", "exam_three", "force_history"), sort_plots = FALSE)+ theme(text=element_text(size=14))
```

## Disturbance-Dependence Plots
	
One common technique for visualizing the adequacy of statistical models in classic regression is residual-dependence plots. With these graphics, one simply plots the residuals of the model ($Y$ axis) against the predicted values ($X$ axis). The rationale behind this is simple: the model should have extracted any association between the prediction and the outcome. The residuals represent the remaining information after extracting the signal from the model. If there is a clear trend remaining in the data (e.g., a nonlinear pattern or a "megaphone" shape in the residuals), this indicates the model failed to capture important information.
	
Likewise, in LVMs, we can apply this same idea to determine whether the fit implied by the LVM has successfully extracted any association between any pair of predictors. However, in LVMs, residuals refer to the discrepancy between the model-implied and the actual variance/covariance matrix (or correlation matrix). As such, naming these plots "residual-dependence plots" would be a misnomer. Rather, misfit at the raw data level is typically called either a disturbance or an individual case residual (ICR), as mentioned previously. In this paper, we call these plots disturbance-dependence plots. 

Like trail plots, we visualize disturbance-dependence plots for each pair of observed variables. To do so, `flexplavaan` subtracts the fit implied by the model from the observed scores. For example, a disturbance dependence plot for an $X_1/X_2$ relationship would subtract the "fit" of $X_2$ implied by the model from the actual $X_2$ scores (and vice versa for the $X_2/X_1$ relationship). If the trail-plot fit actually extracts all association between the pair of observed variables, we would expect to see a scatterplot that shows no remaining association between the two. If there is a pattern in the scatterplot remaining, we know the fit of the model misses information about that specific relationship. 

To aid in interpreting these plots, we can overlay the plot with a flat line (with a slope of zero), as well as a regression (or loess) line. The first line indicates what signal should remain after fitting the model, while the second line shows what actually remains. 

Figure \@ref(fig:ddp) shows an example of trail plots in the upper triangle and disturbance-dependence plots in the lower triangle of a scatterplot matrix. These plots are for the same data shown in the right image of Figure \@ref(fig:trailMatrix2). Notice how many of the plots have nonlinear patterns showing up in both the DDP and the trail plots. 

(ref:crossloadcitation) This plot shows a disturbance-dependence plot for the same data visualized in Figure \@ref(fig:trailMatrix2) in the lower triangle.

```{r ddp, fig.cap='(ref:crossloadcitation)'}
flexplavaan::visualize(force_fit, subset=c("exam_one", "exam_two", "exam_three", "force_history"), sort_plots = FALSE)+ theme(text=element_text(size=14))
```

Together, these plots (hopper, trail plots and DDPs) serve as a critical diagnostic check. All these plots will signal certain types of misfit both in the measurement and structural components of the model. However, these plots suffer from a major weakness. Recall how earlier we referenced Figure \@ref(fig:implied) and noted that sometimes severe misspecification will go undetected simply because an incorrect model will often yield a model-implied covariance matrix that well approximates the actual covariance matrix. However, this sort of misspecification may show up in measurement plots, which we address next. 

## Measurement Plots

Earlier we mentioned how @Asparouhov2017 utilized factor score estimates to visualize ICRs as a diagnostic tool. One of their plots showed the latent variable on the $Y$ axis and the observed (indicator) variable on the $X$ axis. While these may be capable of revealing nonlinearities, they cannot reveal other sorts of misspecification (e.g., many types of cross-loadings and residual correlations) without a simple modification. The modification we propose is similar to the trailplots: overlay the model-implied slope and a regression (or loess) line. 

Fortunately, similar to the trail plots, we can use the model-implied variance/covariance matrix of the observed/latent variables to determine the model-implied slope. The advantage of these plots is they are far more sensitive to many types of misspecification than trail plots. This is because trail plots are unable to pick up misspecification unless that misspecification introduces bias in estimating the observed variance/correlation matrix. However, as shown in Figure \@ref(fig:implied), not all misspecification manifests itself as bias in estimating correlations between observed variables. While the two models in Figure \@ref(fig:implied) will not yield different observed covariances, they will yield different latent variable estimates (and thus, different covariances between latent/observed). 

To create these plots, `flexplavaan` does the following:

1. Estimate factor scores for each individual across each latent variable. 
2. Convert each of the observed variables to z-scores. This allows one to plot multiple observed variables in the same plot. 
3. Use the estimated latent/observed variance/covariance matrix to compute the slope between each observed/latent variable. 

Figure \@ref(fig:measurementplot) plots several graphs of the relationship between the observed variables and the latent variables. `flexplavaan` defaults to displaying only four observed variables at a time. Which four are chosen is determined by the degree of discrepancy between the observed (blue) and model-implied (red) slope, such that the four observed variables with the largest discrepancy are chosen. From Figure \@ref(fig:measurementplot), we see a consistent pattern of nonlinearity between the observed and latent variables that is not captured by the model (shown in red). It is also interesting to note that the history indicator is nearly synonymous with the force latent variable. 

```{r measurementplot, fig.cap='This image, called a measurement plot, shows the relationship between the latent variables ($Y$ axis) and each standardized indicator. The blue lines are loess lines, while the red lines are the model-implied fits of the model.'}
a = implied_measurement(force_fit, latent="Force", method="loess", alpha=.2)[[1]]
b = implied_measurement(force_fit, latent="Jedi", method="loess", alpha=.2)[[1]]
cowplot::plot_grid(a, b)
```


## Structural (Cross-Hair) Plots
<!-- One of the primary purposes of the diagnostics is to determine whether one's conceptualization of the latent variables is to be believed. If the trail plots and disturbance dependence plots indicate the LVM is a good representation of the data, one can be more confident the latent variables are properly estimated. If this is the case, we can now make a step toward visualizing the latent variables themselves.  -->
<!-- Another alteration from a standard scatterplot is the use of vertical bars to indicate uncertainty in estimating the latent variables. In Figure \@ref(measurementplot), the dots represent the estimated factor scores and the lines indicate $\pm 1 SD$. The red line is a "ghost line" [@Fife2019c], which simply repeats the line from the second panel ($x2$) to the other panels. This makes it easier to identify which indicators best load onto the latent variable (i.e., which indicator is most reliable). In Figure \@ref(fig:measurementplot), $x2$ has the largest factor score.  -->

When modeling latent variables, often the visuals of interest are not the observed variables, but the latent variables. In other words, the measurement model is ancillary to the substantive model. Naturally, we might wish to visualize the relationship between the latent variables.

However, latent factor scores computed by LVM software are merely estimates. As such, we ought to have visuals that reflect uncertainty in our estimates of the latent scores. In `flexplavaan`, this uncertainty is represented as crosshairs. The widths of each line of the crosshair (for both the $X$ axis and the $Y$ axis) are obtained from prediction intervals for `lavaan` objects (using the `plausibleValues` function of the `semTools` package). Importantly, the value of factor score estimates is dependent on the degree of factor indeterminacy [@grice2001computing; @rigdon2019factor; @steiger1996dispelling]. Figure \@ref(fig:beech) shows these plots, which we call "structural plots," or "cross-hair plots" with a loess line (in blue). We also added a regression line (in red). Interestingly, the relationship between the two, though simulated to be significantly nonlinear, shows up as fairly linear. (The blue loess line and red regression line are quite similar). This seems to suggest that, by the time a model's factor scores are estimated, any nonlinearity that exists in the data has been discarded. For this reason, we disagree with the approach recommended by @Hallgren2019a, who suggest researchers plot a structural plot as evidence for model fit; simply plotting structural plots without the previous graphics will yield a very misleading picture of data/model fit. This final plot is little more than a summary of the model, provided the model diagnostics check out. Showing these is no more meaningful than reporting a p-value for a model that may or may not meet statistical assumptions. 

```{r beech, fig.cap='Structural or "cross-hair" plot of the relationship between the latent variables force and Jedi. The crosshairs represent the prediction intervals for the factor scores of the latent variables.', echo=FALSE}
flexplavaan::visualize(force_fit, plot="latent", method="loess")[[1]] +
  geom_smooth(method="lm", col="red")

```

When there are multiple latent variables, there is a great deal of flexibility in how one visualizes the structural model. `flexplavaan` makes a best guess at how to visualize this relationship using the model specified by the user. However, the user can always specify how to plot the structural model using a `flexplot` equation [@Fife2019c]. In our example, we only had two variables to visualize, so a simple bivariate plot was most natural. When more variables are included, we might utilize paneling, added variable plots, beeswarm plots, etc. For a review of the types of plots possible, see @Fife2019c. 

## Model Comparisons

It is quite common to compare two different models when using LVMs. Indeed, model comparisons are a recommended strategy for building LVMs [@rodgers_epistemology_2010]. Not only can we use statistics to do model comparisons, but we can also compare them visually. All plots previously mentioned (hopper plots, trail plots, DDPs, and structural plots) can visualize two models at the same time. While visualizing two models side-by-side makes differences more visually detectable, we also recommend visualizing each model individually since certain aspects of misfit in a model might be masked by the comparison. 

For illustration purposes, we're going to fit a second model that attempts to address both the nonlinearity and the missing cross-loading. The second model is shown in Figure \@ref(fig:forceNonlin). To model the nonlinearity, we have created three nonlinear indicators (saber$^2$, fitness$^2$, and midichlorian$^2$), using the "product indicator" approach [@little2006merits]. We have also added the cross-loading from Jedi to history. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[node distance=2cm, midway, scale=1.5, every node/.style={scale=1.5}]

\node[draw, circle] (force) {Force};
\node[draw, circle, node distance = 2cm] (forcesq) [above right of=force] {$\text{Force}^2$};
\node[draw, circle, node distance = 2cm] (jedi) [right of=force] {Jedi};

\node[draw, rectangle] (saber) [left of=force] {Saber};
\node[draw, rectangle, node distance = 1cm] (fit) [above of=saber] {Fitness};
\node[draw, rectangle, node distance = 1cm] (midi) [below of=saber] {Midichlorian};


\node[draw, rectangle] (sabersq) [above of=forcesq] {$\text{Saber}^2$};
\node[draw, rectangle, node distance = 1.6cm] (fitsq) [left of=sabersq] {$\text{Fitness}^2$};
\node[draw, rectangle, node distance = 2.1cm] (midisq) [right of=sabersq] {$\text{Midichlorian}^2`$};


\node[draw, rectangle] (hist) [below of=jedi] {History};

\node[draw, rectangle] (e2) [right of=jedi] {Exam II};
\node[draw, rectangle, node distance = 1cm] (e1) [above of=e2] {Exam I};
\node[draw, rectangle, node distance = 1cm] (e3) [below of=e2] {Exam III};

\draw[->] (force) to node [fill=white]{\scriptsize $b$} (saber);
\draw[->] (force) to node [fill=white]{\scriptsize $a$} (fit);
\draw[->] (force) to node [fill=white]{\scriptsize $c$} (midi);

\draw[->] (forcesq) to node [fill=white]{\scriptsize $b_2$} (sabersq);
\draw[->] (forcesq) to node [fill=white]{\scriptsize $a_2$} (fitsq);
\draw[->] (forcesq) to node [fill=white]{\scriptsize $c_2$} (midisq);

\draw[->] (jedi) to node [fill=white]{\scriptsize $d$} (e1);
\draw[->] (jedi) to node [fill=white]{\scriptsize $e$} (e2);
\draw[->] (jedi) to node [fill=white]{\scriptsize $f$} (e3);

\draw[->] (force) to node [fill=white]{\scriptsize $g_2$} (hist);
\draw[->] (jedi) to node [fill=white]{\scriptsize $g_1$} (hist);

\draw[->, line width = .7mm] (force) to node [below]{\scriptsize $\beta$} (jedi);

\draw[<->] (force.north) edge[out=90, in=180, above] node {$r_1$} (forcesq.west)  ;
\draw[->] (forcesq.south east) to node [above right]{$r_2$} (jedi.north)  ;


\end{tikzpicture}
\caption{Nonlinear model for the Jedi dataset. This model proposes a new latent variable ($\text{Force}^2$) that has saber, fitness, and midichlorian as squared indicators. This model also allows history to load onto both force and Jedi.}
\label{fig:forceNonlin}
\end{center}
\end{figure}

Figure \@ref(fig:hopperModcomp) shows the residual plots, for all pairwise relationships. The new model seems to have reduced the largest residuals (though it has, in some instances, slightly increased the size of some residuals). 

```{r hopperModcomp, fig.cap="A hopper plot, but this time two models are being compared. The red line shows the residuals for the original model, while the blue lines show the residuals for the polynomial model."}
residual_plots(force_fit, nonlinear)
```

Figure \@ref(fig:trailnl) shows the trail/DDPs for the two models. As before, the red line is the original model, while the blue line shows the nonlinear model for the variables saber, fitness, midichlorian, and history. There are some minor discrepancies (e.g., the saber/midichlorian relationship), with the nonlinear model suggesting stronger relationships between the variables. However, as before, these plots are not terribly sensitive to misspecification. (In fact, the trail/DDPs for the other factor showed hardly any discrepancies.) 


```{r trailnl, fig.cap="trail plots and DDPs for the two models. The red shows the original model, while the blue shows the fit for the nonlinear model."}
flexplavaan::visualize(force_fit, nonlinear, subset=c("saber", "fitness", "midichlorian", "force_history")) + theme(text=element_text(size=14))
```

On the other hand, the measurement plots (Figure \@ref(fig:measnl)) do show differences between the two models, at least for the force latent variable. The nonlinear model shows consistently weaker relationships between the force variable and the indicators shown (force history, exam one, exam two, and exam three). 


```{r measnl, fig.cap="Measurement plots for the force latent variable, with both the nonlinear, red, model and original, blue, model."}
implied_measurement(force_fit, nonlinear, latent="Force", alpha = .5)[[1]]
```

Finally, the structural plot, shown in Figure \@ref(fig:structureNonlin) shows the relationship between the force/Jedi variable. The data from the left plot are for the original model, while the right plot are for the nonlinear model. Not surprisingly, the right model captures the nonlinearity between the two much better than does the original model. 

```{r structureNonlin, fig.cap="Relationship between the two latent variables for the original model (left) and the nonlinear model (right). The red line is a 'ghost line,' which simply repeats the line from the left plot to the right plot."}
visualize(force_fit, nonlinear, plot="latent")[[1]]
```

Taken together, these results suggest the original model failed to capture important nonlinear relationship, overestimated some relationships, and underestimated others. The nonlinear model, however, has smaller residuals, seems to better approximate the actual data, and suggests the latent variables are associated nonlinearly. Table \@ref(tab:tabresults) shows fit statistics for each model. We display these *not* to validate the efficacy of the visuals (we trust the visuals more than we trust the fit statistics). Rather, we show these to highlight the convergent validity of the statistics and the graphics. 


```{r tabresults, results='asis'}
fit_o = fitMeasures(force_fit$lavaan) %>% round(digits=3)
fit_n = fitMeasures(nonlinear$lavaan) %>% round(digits=3)
measures = c("chisq", "df", "pvalue", "cfi", "tli", "bic", "rmsea", "srmr")
tab = data.frame(Original=fit_o[measures], Nonlinear=fit_n[measures]) 
tab = data.frame(t(tab))
papaja::apa_table(tab, row.names=T, 
             caption="Global Fit Indices for the Original and Nonlinear Model for the Jedi Dataset.",
             escape = FALSE,
             col.names = c("", "$\\chi^2$", "$df$", "$p$", "CFI", "TLI", "BIC", "RMSEA", "SRMR"),
             align = c("l", rep("c", 8)),
             digits = c(0,1,0,3,3,3,1,3,3))
```

### Comparing Just-Identified/Equivalent Models

As we mentioned previously, simply viewing model-implied variance/covariance matrices may be misleading since two very different models will yield identical variance/covariance matrices (as in Figure \@ref(fig:implied)). Notably, if one were to fit these models, they would be both just-identified and equivalent. (Granted, all just-identified models are also equivalent). However, when we use `flexplavaan` to visualize these models, we see very clear problems. The top plot in Figure \@ref(fig:equivalent_models) shows the measurement plot for the proposed model in the left image of Figure \@ref(fig:implied). Notice how the implied fig (red line) is consistently lower than the regression-based fit (blue line), even though the model is just-identified. This should signal problems that something is wrong with the model. If we were to then fit the right model in Figure \@ref(fig:implied) and compare the two, we would see very model-implied relationships between our variables and the latent factor (particularly for $x3$). 


```{r equivalent_models, fig.cap="Measurement plots of the user-specified model in Figure 2. While fit statistics will not reveal any problems, the measurement plots signal problems. The top plot shows that the model-implied fit consistently underestimates the relationship between the observed and latent variable. The bottom plot compares the true model to the proposed model and reveals the two models say very different things about the relationships between the observed and latent variable."}
a = implied_measurement(ji_model_incorrect, method="lm", latent="f")[[1]]
b = implied_measurement(ji_model_incorrect, ji_model_correct, latent="f", alpha=.2)[[1]]
cowplot::plot_grid(a,b, nrow=2)
```

The reason `flexplavaan` is able to evaluate models that standard LVM machinery cannot is because the degrees of freedom for fit statistics are based on the variance/covariance matrix. The visual degrees of freedom from `flexplavaan`, on the other hand, are based on the *raw data*. While the LVM degrees of freedom only allow six total sources of misfit (a.k.a., degrees of freedom), the raw data allow a total of 1,000 sources of misfit (because $N$ = 1000). To be clear, we don't necessarily recommend routinely fitting just-identified models; plots are no substitute for statistics (and vice versa). However, in situations where a just-identified model is the appropriate model, `flexplavaan` allows one to evaluate these sorts of models. 

While we do not necessarily recommend routinely fitting just-identified models, we do recommend users visualize equivalent models using `flexplavaan.` While the fit statistics cannot inform users about which model is more appropriate, `flexplavaan` can, as shown in Figure \@ref(fig:equivalent_models)

# Applied Example: Social Distress & Wellbeing

To demonstrate the utility of `flexplavaan` with real-world data, we use publicly-accessible data from the NIH Toolbox Norming Study [@toolboxData]. Our primary aim is to estimate the effect of social distress on emotional well-being. We use item response theory (IRT)-based $\theta$ scores to represent overall levels of constructs measured using multi-item questionnaires from the *Emotion Domain* [@salsman2013emotion]. For the purposes of this demonstration, we limit our sample to adults, defined as age 18 or older (*N* = 1,629). Our hypothesized model is depicted in \@ref(fig:toolbox1):

```{r, toolbox1, fig.align='center',out.width='25%', fig.cap="Hypothesized Model of the Effect of Social Distress on Emotional Well-Being with the NIH Toolbox Norming Data Set. In this model, Perceived Rejection and Perceived Hostility are treated as causal indicators of the Social Distress latent factor. Estimated parameters have red labels."}
knitr::include_graphics("toolboxDiagramPrimary.png")
```
The model freely estimates 15 parameters labeled with red Greek letters. The primary response variable, $\eta_{2}$, measures emotional well-being as a latent factor with two reflective indicators: _life satisfaction_ ($y_3$) and _meaning_ ($y_4$). The primary exposure variable is social distress ($\eta_{1}$), measured with two reflective indicators: _loneliness_ ($y_1$) and _sadness_ ($y_2$). Social distress also has two causal indicators, _perceived hostility_ and _rejection_. Rather than assuming these two causal indicators are perfectly measured, we use reliability estimates reported in @cyranowski2013assessing to form single-indicator latent variables ($\xi_1$ and $\xi_2$) with error variances equal to (1-$\alpha_i$)*var($\xi_i$), where $\alpha_i$ is the estimate of reliability for measured variable $i$ [@hayduk2012should]. We allow the _sadness_ reflective indicator to have a direct causal effect on well-being, indicating the hypothesis that the association between sadness and well-being is not merely due to the having social distress as a common cause. 

It is sometimes difficult to distinguish between causal and effect indicators for latent factors [@bollen2011three], and modelers may differ in how they conceptualize the same indicators. \@ref(fig:toolbox2) presents an alternative model in which _perceived hostility_ and _perceived rejection_ are treated as reflective indicators of the _social distress_ factor, rather than causal indicators. In our hypothesized model, changes in perceived hostility and rejection lead to corresponding changes in levels of the social distress latent factor and its effect indicators; however, changes in social distress do not lead to changes in perceived hostility and rejection. In the alternative model, changes in the social distress factor lead to changes in perceived rejection and hostility. This difference between models has important clinical implications. In the hypothesized model, interventions reducing perceived rejection and hostility would lead to improvements in both social distress and well-being. The same is not true in the alternative model where rejection and hostility exert no causal influence on social distress or well-being. The alternative model estimates one fewer parameter than the hypothesized model (14 vs. 15) and, consequently, has one additional $df$. 

```{r, , toolbox2, fig.align='center',out.width='25%', fig.cap="Alternative Model of the Effect of Social Distress on Emotional Well-Being with the NIH Toolbox Norming Data Set. In this model, Perceived Rejection and Perceived Hostility are treated as reflective indicators of the Social Distress latent factor rather than causal indicators. Estimated parameters have red labels."}
knitr::include_graphics("toolboxDiagramAlternative.png")
```

![Alternative Model of Social Distress' Effect on Well-Being\label{fig:alternativeToolbox}](images/toolboxDiagramAlternative.png)

We use `flexplavaan` to evaluate the tenability of these two models. All measured variables are treated as continuous. Preliminary graphical analyses showed evidence of floor and ceiling effects causing considerable skewness and kurtosis for several variables, so we used the [MLR estimator](https://lavaan.ugent.be/tutorial/est.html) in lavaan which provides robust standard errors and a scaled $\chi^2$ test statistic.  

We evaluate the global fit of the competing models using the scaled $\chi^2$ fit test and three approximate fit indices: the RMSEA with a 90% confidence interval, the TLI, and the square root mean residual (SRMR).

```{r warning=F, message=F}

load("EmotionScores.RData")
#load("..flexplavaan/flexplavaan/nihtoolbox.RData")
adult <- subset( x, age >= 18 )

require(lavaan)

hypoth <- '
rejectl =~ 1*PercRejection18_85
PercRejection18_85 ~~ .07*PercRejection18_85
hostilityl =~ 1*PercHostility18_85
PercHostility18_85 ~~ .06*PercHostility18_85
distress =~ Loneliness18_85 + Sad18_85
distress ~ rejectl + hostilityl
well =~ LifeSatisfaction18_85 + Meaning18_85
well ~ b1*distress + Sad18_85
'

hypothfit <- sem( hypoth, data = adult, estimator = 'mlr' )


alternate <- '
distress =~ Loneliness18_85 + PercRejection18_85 + PercHostility18_85 + Sad18_85
well =~ LifeSatisfaction18_85 + Meaning18_85
well ~ b1*distress + Sad18_85
'

alternatefit <- sem( alternate, data = adult, estimator= 'mlr')

fithyp <- round( fitMeasures( hypothfit, c('npar',"chisq", "df", "pvalue", "rmsea",'rmsea.ci.lower','rmsea.ci.upper',"tli",'srmr') ), 2 )
fitalt <- round( fitMeasures( alternatefit, c('npar',"chisq", "df", "pvalue", "rmsea",'rmsea.ci.lower','rmsea.ci.upper',"tli",'srmr') ), 2 )


fitcomp <- data.frame( Model=c('Alternative','Hypothesized'), rbind( fitalt, fithyp ) ) 
papaja::apa_table(fitcomp, row.names=F, 
             caption="Global Fit Indices for the Alternative and Hypothesized Model for the NIH Toolbox Dataset.",
             escape = FALSE,
             col.names = c("Model","parameters", "$\\chi^2$", "$df$", "$p$","RMSEA", "RMSEA$_{l}$", "RMSEA$_{u}$", "TLI", "SRMR"),
             align = c("l", rep("c", 9)),
             digits = c(0,0,1,0,3,3,3,3,3, 3))

```

The scaled $\chi^2$ tests for both models indicate that there are model-data discrepancies that exceed the conventional threshold (*p*=.05) used to distinguish chance vs. non-chance expectation, though the $\chi^2$ test statistic is considerably smaller in the hypothesized model. The approximate fit indices favor the hypothesized model. Using flexplavaan's `residual_plots()` function, we can hopper plots to visualize which bivariate correlations are most poorly approximated by the model. 


```{r message=F, warning=F}
residual_plots( alternatefit, hypothfit )
```

The hypothesized model yields smaller residual correlations for nearly all variable pairs, with the absolute value of all residual correlations < .03. The alternate model does a relatively poor job reproducing the bivariate associations among the indicators of *social distress*. For example, when including *hostility* and *rejection* as reflective indicators, the alternative model underestimates the *lonely-sad* and *reject-hostility* correlations, whereas the hypothesized model reproduces these relationships nearly perfectly. The hypothesized model also generally does better reproducing correlations between the *social distress* indicators and the *well-being* indicators. The alternative model most notably overestimates the *loneliness-meaning* correlation.

At this point, we expect that many SEM users might end their critique of the hypothesized model. Most reviewers in applied journals would accept that the model provides a "close fit" to the data as all approximate fit indices are within conventional thresholds. With *N* > 1000, the significant $\chi^2$ test might be dismissed as overly sensitive, likely detecting negligible discrepancies between the model and data. However, we will pursue visual inspection of the model.

First, we use the `visualize()` function to evaluate trail and disturbance dependence plots for our hypothesized model. To make the plot matrix manageable, we initially include only the indicators for the social distress latent factor using the *subset* argument.   

```{r}
visualize(hypothfit, subset=c('PercHostility18_85','PercRejection18_85','Loneliness18_85','Sad18_85'))
```
The trail plots show floor effects with large clusters of points in the bottom-left corners. Additionally, the loess lines summarizing the observed patterns of association show some evidence of curvature for several variable pairs at high levels of the $X$-axis variables where the data are sparse. There appears to be clusters of outlying points with high leverage pulling the loess lines downward away from the implied linear slope. For example, there appear to be a number of participants reporting high levels of hostility and rejection but low levels of sadness. Several disturbance dependence plots also show clusters of aberrant points contributing to deviations from the model-implied zero slopes. The floor effects also appear to be contributing to heteroscedasticity. 

Similar patterns can be seen when visualizing relationships between the social distress causal indicators (*rejection* and *hostility*) and the the well-being reflective indicators (*meaning* and *satisfaction*). There are departures from the model-implied linear effects, seemingly driven by clusters of aberrant cases (e.g., participants reporting high levels of rejection *and* high levels of meaning). 

```{r}
visualize(hypothfit, subset=c('PercHostility18_85','PercRejection18_85','Meaning18_85','LifeSatisfaction18_85'))
```

We can also view how the hypothesized and alternative models differ in their implications by including both models in the same trail and disturbance dependence plots.

```{r}
visualize(hypothfit, alternatefit, subset=c('PercHostility18_85','PercRejection18_85','Loneliness18_85','Sad18_85'))
```

The models produce generally similar trail and disturbance dependence plots. However, a close evaluation of the trail plots may help  users recognize how the models differ in their implications. The implied correlations between the hypothesized causal indicators and reflective indicators of the *social distress* factor are consistently stronger in the alternative model, where all four indicators are treated as reflective indicators. In contrast, the implied correlations between the two hypothesized causal indicators (*hostility* and *rejection*) and between the the two hypothesized reflective indicators (*sadness* and *loneliness*) are stronger in the hypothesized model.  

Measurement plots for the hypothesized model generally show that the model-implied relationships between the latent and observed variables are consistent with the observed relationships between factor score estimates and the observed variables. For the social distress latent factor, there is some evidence of curvature deviating from the implied linear slope in the sparse regions of the plots. This is particularly notable for the plot of the distress factor score estimates against hostility. This may be at least partly an artifact of the floor effect for the hostility variable, resulting in an underestimate of the effect of hostility on the social distress factor. There also appear to be a cluster of outliers -- individuals reporting low levels of meaning *and* low levels of distress -- pulling the loess line for the plot of distress against meaning away from the implied linear relationship.  
```{r}
implied_measurement(hypothfit, latent='distress', limit=8)[[1]]
```


As many of the `flexplavaan` visualizations showed outlying points that were inconsistent with the model-implied relationships, we use the `fitinfluence()` function in the `influence.SEM` package [@influenceSEM] to identify cases with the greatest influence on the hypothesized model's $\chi^2$ test. The 10 cases with the greatest influence on model fit are plotted below.  


```{r}
require(influence.SEM)
#hypoth.inf <- fitinfluence(model=hypothfit,index = 'chisq', data=adult) #Takes too long to run -- output saved as 'hypoth.inf.RData'
#save( hypoth.inf, file='hypoth.inf.RData' )
load("hypoth.inf.RData")
hypoth.inf.sort <- hypoth.inf$Dind[ order(-abs(hypoth.inf$Dind$chisq)), ]
hypoth.inf.sort1.10 <- hypoth.inf.sort[1:10,]
hypoth.inf.sort1.10$chisqabs <- abs( hypoth.inf.sort1.10$chisq )
hypoth.inf.sort1.10$rank <- factor(1:10)
hypoth.inf.sort1.10$RemovalEffect <- factor( ifelse( hypoth.inf.sort1.10$chisq > 0, 1, 0), levels = 0:1, labels = c('Improved Fit','Worsened Fit') )
infplot <- p<-ggplot(hypoth.inf.sort1.10, aes(x=chisq, y=rank, fill = RemovalEffect)) + 
  geom_dotplot(binaxis='y') +
  labs(title='Highest Leverage Cases', x='Change in Chi-Squared Test Statistic with Case Removal', y='Rank: Cases with Biggest Impact on Fit') + geom_vline(xintercept = 0 )
infplot
```

It can be informative to evaluate data patterns for individuals cases for which the model performs poorly. As shown in the plot, there are cases whose removal would improve model fit and others whose removal would worsen fit. The removal of the case with the largest influence on the model fit (highlighted in green) would result in a reduction (improvement) in the $\chi^2$ value of 2.34. This participant had an unusual pattern of values on the measured variables, scoring 1.58 *SD*s above the sample mean on the *meaning* composite despite scoring 1.70 *SD*s below average on *life satisfaction* and well above the sample means on *loneliness* (+2.70 *SD*s) and *sadness* (+1.90 *SD*s). Importantly, these unusual combinations are theoretically plausible, so there is no reason to assume that the . One could, for example, maintain high levels of meaning despite having poor life satisfaction and high levels of sadness and loneliness. A big discrepancy in *meaning-satisfaction* and *loneliness-sadness* composite scores was common among participants for whom the model performed poorly. This is an indication that we should further refine our theory and model. It may be, for example, preferable to treat *life satisfaction* and *meaning* as separate latent variables rather than as indicators of a single latent factor.

It can be informative to evaluate data patterns for individuals cases for which the model performs poorly. As shown in the plot, there are cases whose removal would improve model fit and others whose removal would worsen fit. The removal of the case with the largest influence on the model fit (highlighted in green) would result in a reduction (improvement) in the $\chi^2$ value of 2.34. This participant had an unusual pattern of values on the measured variables, scoring 1.58 *SD*s above the sample mean on the *meaning* composite despite scoring 1.70 *SD*s below average on *life satisfaction* and well above the sample means on *loneliness* (+2.70 *SD*s) and *sadness* (+1.90 *SD*s). Importantly, these unusual combinations are theoretically plausible. One could, for example, maintain high levels of meaning despite having poor life satisfaction and high levels of sadness and loneliness. A big discrepancy in *meaning-satisfaction* and *loneliness-sadness* composite scores was common among participants for whom the model performed poorly. This is an indication that we should further refine our theory and model. It may be, for example, preferable to treat *life satisfaction* and *meaning* as separate latent variables rather than as indicators of a single latent.

This example demonstrates how `fleplavaan`'s visualization functions can facilitate deeper evaluation of LVMs than numeric fit indices alone. Hopper plots overlaying the residual covariances from our hypothesized and alternative models showed plainly that our hypothesized model better reproduced bivariate associations than the alternative model. Additionally, the Hopper plot showed whether discrepancies were due to over- vs. under-estimation of associations. trail, disturbance dependence, and measurement plots alerted us to potentially problematic distributional properties (i.e., floor and ceiling effects) and high-leverage outliers. Having plotted these variables prior to running our models, we anticipated that normality assumptions were tenuous, prompting us to use a robust estimation procedure; however, the plots showed the potential impact on model estimates. For example, the floor effect for the *hostility* variable may contribute to an underestimate of its effect on the *social distress* latent factor. This example is instructive as it utilized data from real-world instruments with unwieldy distributions common in psychology applications. When used in conjunction with the `influence.SEM` package [@influenceSEM], we identified cases for which the model performed poorly, giving us hypotheses about potential modifications to our theory and model. 

Modelers relying exclusively on conventional cutoffs on the approximate fit indices -- all of which had values indicative of a well-fitting model by conventional standards -- would be unlikely to appreciate the potential shortcomings of the hypothesized model. The scaled $\chi^2$ test, which is often dismissed as being overly sensitive, alerted us to model-data discrepancies. Importantly, `fleplavaan` could be helpful in identifying problems in models even when the $\chi^2$ test is not significant. 



```{r echo=F}
## Show that for individuals for whom the model performed least well had large discrepancies between life satisfaction & meaning 
diffpoor <- adult$satis[ hypoth.inf.sort1.10$case ] - adult$meaning[ hypoth.inf.sort1.10$case ]
diffoth <- adult$satis[ -hypoth.inf.sort1.10$case ] - adult$meaning[ -hypoth.inf.sort1.10$case ]
#summary(diffoth)

## Show that for individuals for whom the model performed least well had large discrepancies between loneliness and sadness
diff1poor <- adult$satis[ hypoth.inf.sort1.10$case ] - adult$meaning[ hypoth.inf.sort1.10$case ]
diff1oth <- adult$sad[ -hypoth.inf.sort1.10$case ] - adult$lonely[ -hypoth.inf.sort1.10$case ]
#summary(diffoth)
```

# Discussion

LVM scholars have long warned about the deficiencies of relying *exclusively* on the $\chi^2$ fit test and approximate fit indices when judging model adequacy [e.g., @tomarken2003potential]. It is well-known that models meeting conventional standards for model adequacy can have serious problems that go undetected, potentially leading to biased estimates and faulty inferences [@hayduk2014seeing; @tomarken2003potential]. The $\chi^2$ test, though more sensitive to misspecifications than approximate fit indices, cannot detect all problems [@hayduk2014shame]. Therefore, regardless of whether there is evidence of misspecification, in-depth model evaluation strategies are needed to identify problems (including those flying under the fit index radar) and to inform subsequent model development. When models perform poorly, numeric fit indices provide, at best, vague hints about what went wrong. A model could provide a poor match to the data because of misspecifications in the causal structure, violations of distributional assumptions, or a combination of both [@bollen_structural_1989]. 

In this paper, we have introduced `fleplavaan`, an open-source R package designed to provide easy-to-use and intuitive visualizations of latent variables. Throughout we have shown various instances where model fit indices failed to capture important misspecification that `flexplavaan` easily captured. We have also showed instances where fit indices signaled problems, but provided very little guidance on how to proceed. `flexplavaan`, on the other hand, revealed these problems strikingly. 


In addition to providing intuitive diagnostic tools, `flexplavaan` also overcomes (or at least mitigates) various problems endemic in LVMs. For example, with equivalent models, fit statistics will be identical, while visualizations will not. Likewise, just-identified models provide do not allow researchers to evaluate fit using traditional indices. The visuals provided by `flexplavaan`, on the other hand, do provide a means to evaluate model adequacy under these conditions. 

Beyond helping LVM modelers, we believe that `fleplavaan` can improve model transparency and allow reviewers and research consumers to better scrutinize LVMs than they could if they had to rely exclusively on summaries of aggregate statistics. In our applied example with the NIH Toolbox data, for example, the visualizations clearly alert the user to potential model shortcomings even when the approximate fit indices would generally be considered to be indicative of a close fit. Thus, data visualizations can make it more difficult for model problems to go undetected by readers.

Hopefully, throughout this paper, we have convinced the reader that existing methods for evaluating model fit have very poorly communicated model strengths and weaknesses. Without these visuals, we have historically been modeling blind, both literally and figuratively. It is our hope that the tools we provide through `flexplavaan` will increase transparency, improve communication, and provide a means to refine the models we use for psychological research. 

A limitations of flexplavaan and, visual modeling approaches more generally, is that they provide no decision threshold for determining whether model-data deviations reflect meaningful misspecifications or non-systematic processes (e.g., sampling error). Modelers will differ in their perceptions of whether visual departures of observed values from their model-based predictions are meaningful or simply noise. On the other hand, conventional decision thresholds are also arbitrary and cannot be relied upon to distinguish problematic from nonproblematic models [@chen2008empirical; @hayduk2014seeing]. Additionally, some of the visualization strategies (e.g., measurement and structural plots) rely on factor scores estimates. In the presence of high factor indeterminacy, different methods factor score estimation methods could be highly discrepant [@grice2001computinggrice]. Evaluating the degree of indeterminacy, for example, using Guttman's index [e.g., @guttman1955determinacy], is recommended when evaluating factor score-based diagnostics.        

# Getting Started with Flexplavaan

`flexplavaan` was designed to be easy-to-use with one-line functions for most visualization strategies. To install `flexplavaan`, users must first download the `devtools` package:

```{r, echo=TRUE, eval=FALSE}
install.packages("devtools")
```

Then, using the devtools package, the `flexplavaan` package can be installed from github:

```{r, echo=TRUE, eval=FALSE}
devtools::install_github("dustinfife/flexplavaan")
```

`flexplavaan` comes pre-loaded with several dataset, including the jedi dataset from this paper. The dataset is called `jedi_jedi` in the `flexplavaan` package. To load and model the dataset, the user could, for example, use the following `lavaan` code:

```{r, echo=TRUE, eval=FALSE}
require(flexplavaan)

model = "
Force =~ fitness + saber + midichlorian + force_history
Jedi =~ exam_one + exam_two + exam_three
Jedi ~ Force
"

# Fit the models ----------------------------------------------------------
force_fit = flexplavaan(model, jedi_jedi)
```

Once the user has the fitted `lavaan` object, they may use visuals in `flexplavaan`, as in the following code:

```{r, echo=TRUE, eval=FALSE}
# show the hopper plots, only including those variables that have a residual larger than 0.01
residual_plots(force_fit, max_val = .01)

# trail/ddp of the first 4 variables
visualize(force_fit, subset=1:4)

# alternatively, specify which variables should be plotted
visualize(force_fit, subset=c("fitness", "midichlorian", "exam_one"))

# show the measurement plot for the Force latent variable
implied_measurement(force_fit, latent="Force")

# show the structural plot, with Force on the Y axis. 
visualize(force_fit, plot="model", formula = Force~Jedi)
```

The source code for `flexplavaan` is available on github at github.com/dustinfife/flexplavaan, as well as the source code that generated this document. 


# References